{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- NOTES -------------------------------------------------------------------\n",
    "# 1. Update the datasets, dataList\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pyspark\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import DoubleType\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.sql.functions import udf, unix_timestamp, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Input & output directories\n",
    "\n",
    "    #file_list = \"../cluster3.txt\"\n",
    "    file_list = \"/home/ted/school/big_data/project/big_data_course_project/cluster3.txt\"\n",
    "\n",
    "    # use these if running locally (Jupyter, etc)\n",
    "    inputDirectory = \"../raw_data/\"\n",
    "    outputDirectory = \"../output_data/\"#sys.argv[2]\n",
    "    \n",
    "    # use these if running on DUMBO\n",
    "    #inputDirectory = \"/user/hm74/NYCColumns/\"#sys.argv[1]\n",
    "    #outputDirectory = \"/user/\" + this_user + \"/project/task1/\"#sys.argv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Setting spark context and \n",
    "    sc = SparkContext()\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"project_task1\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
    "\n",
    "    # Current user path\n",
    "    env_var = os.environ\n",
    "    this_user = env_var['USER']\n",
    "\n",
    "    # Input & output directories. Use these if running on DUMBO\n",
    "    #inputDirectory = \"/user/hm74/NYCOpenData/\"#sys.argv[1]\n",
    "    #outputDirectory = \"/user/\" + this_user + \"/project/task1/\"#sys.argv[2]\n",
    "    \n",
    "    # use these if running locally (Jupyter, etc)\n",
    "    inputDirectory = \"../raw_data/\"\n",
    "    outputDirectory = \"../output_data/\"#sys.argv[2]\n",
    "\n",
    "    # Output JSON Semantic Schema\n",
    "    semanticSchema = {\n",
    "        \"semantic_type\": \"\",\n",
    "        \"count\": 0\n",
    "    }\n",
    "\n",
    "    # Define of different types regex dict:\n",
    "    expr_dic = {\"Street\": \"ROAD|STREET|PLACE|DRIVE|BLVD|%ST%|%RD%|DR|AVENUE\",\n",
    "                \"Website\" : \"WWW.|.COM|HTTP://\",\n",
    "                \"BuildingCode\" : \"([A-Z])\\d\\-\",\n",
    "                \"PhoneBumber\":\"\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d|\\(\\d\\d\\d\\)\\d\\d\\d\\d\\d\\d\\d|\\d\\d\\d\\-\\d\\d\\d\\-\\d\\d\\d\\d\",\n",
    "                \"ZipCode\":\"\\d\\d\\d\\d\\d|\\d\\d\\d\\d\\d\\-\\d\\d\\d|\\d\\d\\d\\d\\d\\d\\d\\d\",\n",
    "                \"Lat_Lon\" : \"\\([-+]?[0-9]+\\.[0-9]+\\,\\s*[-+]?[0-9]+\\.[0-9]+\\)\",\n",
    "                \"SchoolName\" : \"SCHOOL|ACADEMY|HS|ACAD|I.S.|IS|M.S.|P.S|PS|YABC\",\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset =========== :  1 - faiq-9dfq.Vehicle_Color.txt.gz\n",
      "After Scaling :\n",
      "+-----+-------+------------+\n",
      "|  val|  count|count_Scaled|\n",
      "+-----+-------+------------+\n",
      "|   WH|2204226|         1.0|\n",
      "|   GY|1930859|       0.876|\n",
      "|   BK|1714273|       0.778|\n",
      "|WHITE|1243122|       0.564|\n",
      "|   BL| 670601|       0.304|\n",
      "|BLACK| 524911|       0.238|\n",
      "|   RD| 430013|       0.195|\n",
      "| GREY| 347729|       0.158|\n",
      "|BROWN| 314730|       0.143|\n",
      "|SILVE| 223641|       0.101|\n",
      "| BLUE| 207777|       0.094|\n",
      "|  RED| 185461|       0.084|\n",
      "|   GR| 181040|       0.082|\n",
      "|   TN| 118904|       0.054|\n",
      "|   YW|  94288|       0.043|\n",
      "|OTHER|  91116|       0.041|\n",
      "|  BLK|  90186|       0.041|\n",
      "|   BR|  88584|        0.04|\n",
      "|GREEN|  73344|       0.033|\n",
      "|   GL|  52874|       0.024|\n",
      "+-----+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # Importing cluster3 format it and put it into a list\n",
    "    raw_data = sc.textFile(\"cluster3.txt\")\n",
    "    raw_list = raw_data.flatMap(lambda x: x.split(\",\")).collect()\n",
    "    raw_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in raw_list]\n",
    "    raw_list = [re.sub(\" \" \"\", \"\", item)for item in raw_list]\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Iteration over dataframes begins bu using dataframe file names\n",
    "    processCount = 1\n",
    "\n",
    "    # Create schema for raw data before reading into df \n",
    "    customSchema = StructType([\n",
    "               StructField(\"val\", StringType(), True),\n",
    "               StructField(\"count\", IntegerType(), True)])\n",
    "    #Testing first 50 files\n",
    "    for filename in raw_list[2:3]:\n",
    "\n",
    "        print(\"Processing Dataset =========== : \", str(processCount) + ' - ' +filename)\n",
    "        df = sqlContext.read.format(\"csv\").option(\"header\",\n",
    "        \"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \n",
    "        \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "\n",
    "        # Count all val in df with count \n",
    "        count_all = df.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        \n",
    "        \n",
    "        # note: the following scaler process is taken from:\n",
    "        # https://stackoverflow.com/questions/40337744/scalenormalise-a-column-in-spark-dataframe-pyspark\n",
    "        # UDF for converting column type from vector to double type\n",
    "        unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "       \n",
    "        for i in [\"count\"]:\n",
    "            # VectorAssembler Transformation - Converting column to vector type\n",
    "            assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "            # MinMaxScaler Transformation\n",
    "            scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "\n",
    "            # Pipeline of VectorAssembler and MinMaxScaler\n",
    "            pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "            # Fitting pipeline on dataframe\n",
    "            df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "\n",
    "        print(\"After Scaling :\")\n",
    "        df.sort(col(\"count\").desc()).show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for k,v in expr_dic.items():\n",
    "            df_filtered = df.filter(df[\"val\"].rlike(v))\n",
    "            if (df_filtered.count() is not 0):\n",
    "                count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "                #print(\"count_filtered:\",count_filtered)\n",
    "                percentage = float(count_filtered/count_all)\n",
    "    #             print(\"percentage of rows contains:\", v , \"is\",percentage )\n",
    "                #Can change the percentage threshold later \n",
    "                if (percentage > 0.85) :\n",
    "                    print(\"semantic type is\" , k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\" END HERE\"\"\"\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataframe from HDFS with datasetnames\n",
    "    datasets = sqlContext.read.format(\"csv\").option(\"header\", \n",
    "        \"false\").option(\"delimiter\", \"\\t\").load(inputDirectory + \"datasets.tsv\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataset file names\n",
    "    dataList = [str(row._c0) for row in datasets.select('_c0').collect()]\n",
    "    # Iteration over dataframes begins bu using dataframe file names\n",
    "    processCount = 1\n",
    "    for filename in dataList[0:1]:\n",
    "        #filename = '833y-fsy8'\n",
    "        print(\"Processing Dataset =========== : \", str(processCount) + ' - ' +filename)\n",
    "        df = sqlContext.read.format(\"csv\").option(\"header\",\n",
    "        \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \n",
    "            \"\\t\").load(inputDirectory + filename + \".txt.gz\")\n",
    "        # Reading the task1 JSON\n",
    "        outJSON = sc.textFile(outputDirectory + filename + '.json')\n",
    "        outJSON = json.load(outJSON.collect()[0])\n",
    "        # Spark SQL view\n",
    "        df.createOrReplaceTempView(\"df\")\n",
    "        # Datatypes dictionary from InferSchema\n",
    "        df_dtypes = {i:j for i,j in df.dtypes}\n",
    "        # Copy of semantic types schema\n",
    "        sem_types = deepcopy(semanticSchema)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # --- ENTER FUNCTION CALLS FROM HERE ----------------------------------\n",
    "\n",
    "        # Finding \"colomns\" attribute for each column\n",
    "        print(\"Number of Columns ============ : \", len(df.columns))\n",
    "        columnCount = 1\n",
    "        for coln in df.columns:\n",
    "            print(\"Processing Column ============ : \", str(columnCount) + ' - ' + coln)\n",
    "            col_type = df_dtypes[coln]\n",
    "            # Handle integers decimal(10,0)\n",
    "            if (col_type in ['int', 'bigint', 'tinyint', 'smallint']) or (('decimal' in col_type) and col_type[-2]=='0'):\n",
    "                #print('1 '+col_type)\n",
    "                pass\n",
    "            # Handle real numbers\n",
    "            elif (col_type in ['float', 'double']) or (('decimal' in col_type) and col_type[-2]!='0'):\n",
    "                #print('2 '+col_type)\n",
    "                pass\n",
    "            # Handle timestamps\n",
    "            elif col_type in ['timestamp', 'date', 'time', 'datetime']:\n",
    "                #print('3 '+col_type)\n",
    "                pass\n",
    "            # Handle strings \n",
    "            elif col_type in ['string', 'boolean']:\n",
    "                #print('4 '+col_type)\n",
    "                pass\n",
    "            else:\n",
    "                #print('NOT FOUND' +col_type)\n",
    "                pass\n",
    "\n",
    "        columnCount+=1\n",
    "        \n",
    "        # USE ME to append all semantic information to the JSON\n",
    "            for i in range(len(outJSON[\"columns\"])):\n",
    "                if outJSON[\"columns\"][i][\"column_name\"]== coln:\n",
    "                    outJSON[\"columns\"][i][\"semantic_types\"].append(sem_types)\n",
    "\n",
    "        # --- FUNCTION CALLS END HERE -----------------------------------------\n",
    "        # ---------------------------------------------------------------------\n",
    "        \n",
    "        # USE ME to export the JSON for current dataset\n",
    "        print(\"Saving Dataset =============== : \", str(processCount) + ' - ' +filename)\n",
    "        processCount += 1\n",
    "        outJSON = sc.parallelize([json.dumps(outJSON)])\n",
    "        outJSON.saveAsTextFile(outputDirectory + filename + '.json')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

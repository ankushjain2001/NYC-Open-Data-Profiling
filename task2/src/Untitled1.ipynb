{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- NOTES -------------------------------------------------------------------\n",
    "# 1. Update the datasets, dataList\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pyspark\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "#from pyspark.sql.types import StructType\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.sql.functions import udf, unix_timestamp, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline \n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "# -----------------------------------------------------------------------------\n",
    "# --- Function Definitions Begin ----------------------------------------------\n",
    "\n",
    "# Function to find meand and stdv for all files\n",
    "def mean_stdv(df):\n",
    "    unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "    for i in [\"count\"]:\n",
    "        assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "        scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "        pipeline = Pipeline(stages=[assembler, scaler])\n",
    "        df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "        df_stats = df.select(_mean(col('count_Scaled')).alias('mean'),_stddev(col('count_Scaled')).alias('std')).collect()\n",
    "        mean = df_stats[0]['mean']\n",
    "        std = df_stats[0]['std']\n",
    "        return df_stats \n",
    "\n",
    "# Function to sum all count of values for all files\n",
    "def count_all_values(df):\n",
    "    res = df.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "    return res\n",
    "\n",
    "# Regex function to check website type\n",
    "def re_find_website(df,count_all,found_type):\n",
    "    web_re_rexpr = \"WWW\\.|\\.COM|HTTP\\:\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(web_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + \"website\"\n",
    "        return res, found_type\n",
    "    else:\n",
    "        return 0, found_type\n",
    "\n",
    "# Regex function to check zip type\n",
    "def re_find_zipCode(df,count_all,found_type):\n",
    "    zip_re_rexpr = \"\\d\\d\\d\\d\\d|\\d\\d\\d\\d\\d\\-\\d\\d\\d|\\d\\d\\d\\d\\d\\d\\d\\d\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(zip_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + \"zipCode\"\n",
    "        return res, found_type\n",
    "    else:\n",
    "        return 0, found_type\n",
    "\n",
    "# Regex function to check buildingCode type\n",
    "def re_find_buildingCode(df,count_all,found_type):\n",
    "    bc_re_rexpr = \"([A-Z])\\d\\-\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(bc_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + \"buildingCode\"\n",
    "        return res, found_type\n",
    "    else:\n",
    "        return 0, found_type\n",
    "\n",
    "# Regex function to check phone number type\n",
    "def re_find_phoneNum(df,count_all,found_type):\n",
    "    phone_re_rexpr = \"\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d|\\(\\d\\d\\d\\)\\d\\d\\d\\d\\d\\d\\d|\\d\\d\\d\\-\\d\\d\\d\\-\\d\\d\\d\\d\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(phone_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + \"phoneNum\"\n",
    "        return res, found_type\n",
    "    else:\n",
    "        return 0, found_type\n",
    "\n",
    "# Regex function to check lat_lon type\n",
    "def re_find_lat_lon(df,count_all,found_type):\n",
    "    ll_re_rexpr = \"\\([-+]?[0-9]+\\.[0-9]+\\,\\s*[-+]?[0-9]+\\.[0-9]+\\)\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(ll_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + \"lat_lon\"\n",
    "        return res, found_type\n",
    "    else:\n",
    "        return 0, found_type\n",
    "\n",
    "# Regex function to check street_addrees type\n",
    "def re_find_street_address(df,count_all,found_type):\n",
    "    st_re_rexpr = \"\\sROAD|\\sSTREET|\\sPLACE|\\sDRIVE|\\sBLVD|\\sST|\\sRD|\\sDR|\\sAVENUE\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(st_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.5): \n",
    "            found_type = found_type + \"street/address\"\n",
    "        return res, found_type\n",
    "    else:\n",
    "        return 0, found_type\n",
    "\n",
    "# Regex function to check school name type\n",
    "def re_find_school(df,count_all,found_type):\n",
    "    school_re_rexpr = \"SCHOOL|ACADEMY|HS\\s|ACAD|I.S.\\s|IS\\s|M.S.\\s|P.S\\s|PS\\s\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(school_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.5): \n",
    "            found_type = found_type + \"school name\"\n",
    "        return res, found_type\n",
    "    else:\n",
    "        return 0, found_type\n",
    "\n",
    "# --- Function Definitions End ------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# --- MAIN --------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setting spark context and \n",
    "    sc = SparkContext()\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"project_task1\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
    "\n",
    "    # Current user path\n",
    "    env_var = os.environ\n",
    "    this_user = env_var['USER']\n",
    "\n",
    "    # Input & output directories\n",
    "    #inputDirectory = \"/user/hm74/NYCOpenData/\"#sys.argv[1]\n",
    "    #outputDirectory = \"/user/\" + this_user + \"/project/task1/\"#sys.argv[2]\n",
    "    \n",
    "    # use these if running locally (Jupyter, etc)\n",
    "    inputDirectory = \"../raw_data/\"\n",
    "    outputDirectory = \"../output_data/\"#sys.argv[2]\n",
    "\n",
    "    # Output JSON Semantic Schema\n",
    "    semanticSchema = {\n",
    "        \"semantic_type\": \"\",\n",
    "        \"count\": 0\n",
    "    }\n",
    "\n",
    "    # Importing cluster3 format it and put it into a list\n",
    "    raw_data = sc.textFile(\"cluster3.txt\")\n",
    "    raw_list = raw_data.flatMap(lambda x: x.split(\",\")).collect()\n",
    "    raw_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in raw_list]\n",
    "    raw_list = [re.sub(\" \" \"\", \"\", item)for item in raw_list]\n",
    "    \n",
    "    # Iteration over dataframes begins bu using dataframe file names\n",
    "    processCount = 1\n",
    "\n",
    "    # Create schema for raw data before reading into df \n",
    "    customSchema = StructType([\n",
    "               StructField(\"val\", StringType(), True),\n",
    "               StructField(\"count\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset =========== :  2 - 5694-9szk.Business_Website_or_Other_URL.txt.gz\n",
      "+--------------------+--------------------+--------------------+---------+------------------+-------------------+-----------------------+-------------------+------------------+--------------------+---------------------+-------+\n",
      "|            filename|                mean|                stdv|count_all|precentage_website|     precentage_zip|percentage_buildingCode|percentage_phoneNum|percentage_lat_lon|   percentage_add_st|percentage_add_school|  types|\n",
      "+--------------------+--------------------+--------------------+---------+------------------+-------------------+-----------------------+-------------------+------------------+--------------------+---------------------+-------+\n",
      "|5694-9szk.Busines...|0.006258692628650904|0.049833904822194255|     1474|0.9972862957937585|0.05495251017639077|                      0|0.04748982360922659|                 0|6.784260515603799E-4| 0.008141112618724558|website|\n",
      "+--------------------+--------------------+--------------------+---------+------------------+-------------------+-----------------------+-------------------+------------------+--------------------+---------------------+-------+\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    regex_res = []\n",
    "    #Testing first 50 files\n",
    "    for filename in raw_list[0:1]:\n",
    "        print(\"Processing Dataset =========== : \", str(processCount) + ' - ' +filename)\n",
    "        # Read file to dataset and apply all regex functions\n",
    "        found_type = \"\"\n",
    "        fileinfo = []\n",
    "        df = sqlContext.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "        df_stats = mean_stdv(df)\n",
    "        mean = df_stats[0]['mean']\n",
    "        std = df_stats[0]['std']\n",
    "        count_all = count_all_values(df)\n",
    "        precentage_website, found_type = re_find_website(df,count_all,found_type)\n",
    "        precentage_zip, found_type= re_find_zipCode(df,count_all,found_type)\n",
    "        percentage_buildingCode, found_type = re_find_buildingCode(df,count_all,found_type)\n",
    "        percentage_phoneNum, found_type = re_find_phoneNum(df,count_all,found_type)\n",
    "        percentage_lat_lon, found_type = re_find_lat_lon(df,count_all,found_type)\n",
    "        percentage_add_st, found_type = re_find_street_address(df,count_all,found_type)\n",
    "        percentage_add_school, found_type = re_find_school(df,count_all,found_type)\n",
    "        fileinfo.extend([filename,mean,std,count_all,precentage_website, precentage_zip, percentage_buildingCode, percentage_phoneNum,percentage_lat_lon,percentage_add_st,percentage_add_school,found_type])\n",
    "        regex_res.append(fileinfo)\n",
    "\n",
    "    # Output regex function result \n",
    "    rdd = sc.parallelize(regex_res)\n",
    "    row_rdd = rdd.map(lambda x: Row(x))\n",
    "    df = row_rdd.toDF()\n",
    "    df = df.select(col('_1').alias('coln'))\n",
    "    length = len(df.select('coln').take(1)[0][0])\n",
    "    df = df.select([df.coln[i] for i in range(length)])\n",
    "    df = df.select(col('coln[0]').alias('filename'),col('coln[1]').alias('mean'),col('coln[2]').alias('stdv'),\n",
    "                   col('coln[3]').alias('count_all'),col('coln[4]').alias('precentage_website'),col('coln[5]').alias('precentage_zip'),\n",
    "                   col('coln[6]').alias('percentage_buildingCode'),col('coln[7]').alias('percentage_phoneNum'),col('coln[8]').alias('percentage_lat_lon'),\n",
    "                   col('coln[9]').alias('percentage_add_st'),col('coln[10]').alias('percentage_add_school'),col('coln[11]').alias('types'),\n",
    "                   )\n",
    "    df.show()\n",
    "\n",
    "    types_found_count = df.where(col('types') > \" \").count()\n",
    "    print(types_found_count)\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "    # USE ME to export the JSON for current dataset\n",
    "    #print(\"Saving Dataset =============== : \", str(processCount) + ' - ' +filename)\n",
    "    #processCount += 1\n",
    "    #outJSON = sc.parallelize([json.dumps(outJSON)])\n",
    "    #outJSON.saveAsTextFile(outputDirectory + filename + '.json')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

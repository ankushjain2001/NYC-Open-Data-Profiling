{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- NOTES -------------------------------------------------------------------\n",
    "# 1. Update the datasets, dataList\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pyspark\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.sql.functions import udf, unix_timestamp, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# --- Function Definitions Begin ----------------------------------------------\n",
    "\n",
    "\n",
    "# --- Function Definitions End ------------------------------------------------\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# --- MAIN --------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setting spark context and \n",
    "    sc = SparkContext()\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"project_task1\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
    "\n",
    "    # Current user path\n",
    "    env_var = os.environ\n",
    "    this_user = env_var['USER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Input & output directories\n",
    "\n",
    "    file_list = \"../cluster3.txt\"\n",
    "\n",
    "    # use these if running locally (Jupyter, etc)\n",
    "    inputDirectory = \"../raw_data/\"\n",
    "    outputDirectory = \"../output_data/\"#sys.argv[2]\n",
    "    \n",
    "    # use these if running on DUMBO\n",
    "    #inputDirectory = \"/user/hm74/NYCColumns/\"#sys.argv[1]\n",
    "    #outputDirectory = \"/user/\" + this_user + \"/project/task1/\"#sys.argv[2]\n",
    "\n",
    "    # Output JSON Semantic Schema\n",
    "    semanticSchema = {\n",
    "        \"semantic_type\": \"\",\n",
    "        \"count\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define of different types regex list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataframe from HDFS with datasetnames\n",
    "    datasets = sqlContext.read.format(\"csv\").option(\"header\", \n",
    "        \"false\").option(\"delimiter\", \"\\t\").load(inputDirectory + \"datasets.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataset file names\n",
    "    dataList = [str(row._c0) for row in datasets.select('_c0').collect()]\n",
    "    # Iteration over dataframes begins bu using dataframe file names\n",
    "    processCount = 1\n",
    "    for filename in dataList[0:1]:\n",
    "        #filename = '833y-fsy8'\n",
    "        print(\"Processing Dataset =========== : \", str(processCount) + ' - ' +filename)\n",
    "        df = sqlContext.read.format(\"csv\").option(\"header\",\n",
    "        \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \n",
    "            \"\\t\").load(inputDirectory + filename + \".tsv.gz\")\n",
    "        # Reading the task1 JSON\n",
    "        outJSON = sc.textFile(outputDirectory + filename + '.json')\n",
    "        outJSON = json.load(outJSON.collect()[0])\n",
    "        # Spark SQL view\n",
    "        df.createOrReplaceTempView(\"df\")\n",
    "        # Datatypes dictionary from InferSchema\n",
    "        df_dtypes = {i:j for i,j in df.dtypes}\n",
    "        # Copy of semantic types schema\n",
    "        sem_types = deepcopy(semanticSchema)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # --- ENTER FUNCTION CALLS FROM HERE ----------------------------------\n",
    "\n",
    "        # Finding \"colomns\" attribute for each column\n",
    "        print(\"Number of Columns ============ : \", len(df.columns))\n",
    "        columnCount = 1\n",
    "        for coln in df.columns:\n",
    "            print(\"Processing Column ============ : \", str(columnCount) + ' - ' + coln)\n",
    "            col_type = df_dtypes[coln]\n",
    "            # Handle integers decimal(10,0)\n",
    "            if (col_type in ['int', 'bigint', 'tinyint', 'smallint']) or (('decimal' in col_type) and col_type[-2]=='0'):\n",
    "                #print('1 '+col_type)\n",
    "                pass\n",
    "            # Handle real numbers\n",
    "            elif (col_type in ['float', 'double']) or (('decimal' in col_type) and col_type[-2]!='0'):\n",
    "                #print('2 '+col_type)\n",
    "                pass\n",
    "            # Handle timestamps\n",
    "            elif col_type in ['timestamp', 'date', 'time', 'datetime']:\n",
    "                #print('3 '+col_type)\n",
    "                pass\n",
    "            # Handle strings \n",
    "            elif col_type in ['string', 'boolean']:\n",
    "                #print('4 '+col_type)\n",
    "                pass\n",
    "            else:\n",
    "                #print('NOT FOUND' +col_type)\n",
    "                pass\n",
    "\n",
    "        columnCount+=1\n",
    "        \n",
    "        # USE ME to append all semantic information to the JSON\n",
    "            for i in range(len(outJSON[\"columns\"])):\n",
    "                if outJSON[\"columns\"][i][\"column_name\"]== coln:\n",
    "                    outJSON[\"columns\"][i][\"semantic_types\"].append(sem_types)\n",
    "\n",
    "        # --- FUNCTION CALLS END HERE -----------------------------------------\n",
    "        # ---------------------------------------------------------------------\n",
    "        \n",
    "        # USE ME to export the JSON for current dataset\n",
    "        print(\"Saving Dataset =============== : \", str(processCount) + ' - ' +filename)\n",
    "        processCount += 1\n",
    "        outJSON = sc.parallelize([json.dumps(outJSON)])\n",
    "        outJSON.saveAsTextFile(outputDirectory + filename + '.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- NOTES -------------------------------------------------------------------\n",
    "# 1. Update the datasets, dataList\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pyspark\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.sql.functions import udf, unix_timestamp, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, TimestampType\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline \n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import array\n",
    "import pyspark.sql.functions as f\n",
    "import csv\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import * \n",
    "from pyspark.sql.types import *\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# --- Function Definitions Begin ----------------------------------------------\n",
    "\n",
    "def write_keyword_list_to_txt(lst, dest_filename):\n",
    "    with open(dest_filename, 'w') as f:\n",
    "        wr = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(lst)\n",
    "\n",
    "# --- Function Definitions End ------------------------------------------------\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# --- MAIN --------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setting spark context and \n",
    "    sc = SparkContext()\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"project_task1\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Current user path\n",
    "    env_var = os.environ\n",
    "    this_user = env_var['USER']\n",
    "\n",
    "    # Input & output directories\n",
    "    #inputDirectory = \"/user/hm74/NYCOpenData/\"#sys.argv[1]\n",
    "    inputDirectory = \"/home/ted/school/big_data/project/big_data_course_project/task2/raw_data/\"\n",
    "    inputFileClusters = \"/home/ted/school/big_data/project/big_data_course_project/task2/resources/filename_clusters.json\"\n",
    "    #outputDirectory = \"/user/\" + this_user + \"/project/task1/\"#sys.argv[2]\n",
    "\n",
    "    # Output JSON Semantic Schema\n",
    "    semanticSchema = {\n",
    "        \"semantic_type\": \"\",\n",
    "        \"count\": 0\n",
    "    }\n",
    "\n",
    "    # Importing cluster3 format it and put it into a list\n",
    "    raw_data = sc.textFile(\"cluster3.txt\")\n",
    "    raw_list = raw_data.flatMap(lambda x: x.split(\",\")).collect()\n",
    "    raw_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in raw_list]\n",
    "    raw_list = [re.sub(\" \" \"\", \"\", item)for item in raw_list]\n",
    "    \n",
    "    # Iteration over dataframes begins bu using dataframe file names\n",
    "    processCount = 1\n",
    "\n",
    "    #df_new = df_split_words.withColumn(\"word\", array(df_split_words[\"word\"]))\n",
    "\n",
    "    # Create schema for raw data before reading into df \n",
    "    customSchema = StructType([\n",
    "        #StructField(\"My_array\", ArrayType(\n",
    "        #StructType([StructField(\"val\", StringType())]))   \n",
    "        StructField(\"val\", StringType(), True),\n",
    "        StructField(\"count\", IntegerType(), True)])    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area of study filelist (aos)\n",
    "aos_list = sc.textFile(\"areas_of_study\")\n",
    "aos_list = aos_list.flatMap(lambda x: x.split(\",\")).collect()\n",
    "aos_list = [x.strip('\"') for x in aos_list]\n",
    "aos_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in aos_list]\n",
    "aos_list = [re.sub(\" \" \"\", \"\", item)for item in aos_list]\n",
    "\n",
    "# generate keyword list for areas of study\n",
    "aos_keyword_list = []\n",
    "aos_current_list = []\n",
    "for filename in aos_list: # first. change to map later\n",
    "    aos_df = sqlContext.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "    aos_df_clean = aos_df.select(\"val\", f.regexp_replace(f.col(\"val\"), \"[\\$#&,]\", \"\").alias(\"clean_word\"))\n",
    "\n",
    "    # split words in each row and create new df with one word per row, and count\n",
    "    aos_df_split_words = aos_df_clean.withColumn('word', f.explode(f.split(f.col('clean_word'), ' ')))\\\n",
    "        .groupBy('word')\\\n",
    "        .count()\\\n",
    "        .sort('count', ascending=False)\\\n",
    "        .filter(\"word != ''\")\n",
    "        \n",
    "    aos_current_list = [row['word'] for row in aos_df_split_words.take(20)]\n",
    "    aos_keyword_list = list(set(aos_keyword_list).union(set(aos_current_list)))\n",
    "    \n",
    "# remove stopwords\n",
    "filtered_aos_keywords = [word for word in aos_keyword_list if word not in stopwords.words('english')]\n",
    "\n",
    "# write to file\n",
    "write_keyword_list_to_txt(filtered_aos_keywords, 'area_of_study_keywords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_aos_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city agency filelist (ca)\n",
    "ca_list = sc.textFile(\"agency_filelist\")\n",
    "ca_list = ca_list.flatMap(lambda x: x.split(\",\")).collect()\n",
    "ca_list = [x.strip('\"') for x in ca_list]\n",
    "ca_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in ca_list]\n",
    "ca_list = [re.sub(\" \" \"\", \"\", item)for item in ca_list]\n",
    "\n",
    "# generate keyword list for city agencies\n",
    "ca_current_list = []\n",
    "ca_keyword_list = []\n",
    "for filename in ca_list: # first. change to map later\n",
    "    ca_df = sqlContext.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "    ca_df_clean = ca_df.select(\"val\", f.regexp_replace(f.col(\"val\"), \"[\\-$#&,]\", \"\").alias(\"clean_word\"))\n",
    "\n",
    "    # split words in each row and create new df with one word per row, and count\n",
    "    ca_df_split_words = ca_df_clean.withColumn('word', f.explode(f.split(f.lower(f.col('clean_word')), ' ')))\\\n",
    "        .groupBy('word')\\\n",
    "        .count()\\\n",
    "        .sort('count', ascending=False)\\\n",
    "        .filter(\"word != ''\")\n",
    "\n",
    "    #ca_df_new = ca_df_split_words.withColumn(\"word\", array(ca_df_split_words[\"word\"]))\n",
    "       \n",
    "    ca_current_list = [row['word'] for row in ca_df_split_words.take(30)]\n",
    "    ca_keyword_list = list(set(ca_keyword_list).union(set(ca_current_list)))\n",
    "\n",
    "# remove stopwords (sw list is lowercase)\n",
    "filtered_ca_keywords = [word for word in ca_keyword_list if word not in stopwords.words('english')]\n",
    "\n",
    "# back to uppercase to match \n",
    "filtered_ca_keywords = [x.upper() for x in filtered_ca_keywords]\n",
    "\n",
    "write_keyword_list_to_txt(filtered_ca_keywords, 'city_agency_keywords')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parks and playgrounds (pp) filelist\n",
    "pp_list = sc.textFile(\"park_playground_filelist\")\n",
    "pp_list = pp_list.flatMap(lambda x: x.split(\",\")).collect()\n",
    "pp_list = [x.strip('\"') for x in pp_list]\n",
    "pp_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in pp_list]\n",
    "pp_list = [re.sub(\" \" \"\", \"\", item)for item in pp_list]\n",
    "\n",
    "# generate keyword list for city agencies\n",
    "pp_current_list = []\n",
    "pp_keyword_list = []\n",
    "for filename in pp_list: # first. change to map later\n",
    "    pp_df = sqlContext.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "    pp_df_clean = pp_df.select(\"val\", f.regexp_replace(f.col(\"val\"), \"[\\$#&,]\", \"\").alias(\"clean_word\"))\n",
    "        \n",
    "    # split words in each row and create new df with one word per row, and count\n",
    "    pp_df_split_words = pp_df_clean.withColumn('word', f.explode(f.split(f.lower(f.col('val')), ' ')))\\\n",
    "        .groupBy('word')\\\n",
    "        .count()\\\n",
    "        .sort('count', ascending=False)\\\n",
    "        .filter(\"word != ''\")\n",
    "    #pp_df_new = pp_df_split_words.withColumn(\"word\", array(pp_df_split_words[\"word\"]))\n",
    "    \n",
    "        \n",
    "    pp_current_list = [row['word'] for row in pp_df_split_words.take(40)]\n",
    "    pp_keyword_list = list(set(pp_keyword_list).union(set(pp_current_list)))\n",
    "\n",
    "# remove stopwords and other words which may result in a missclassification\n",
    "# TODO: clean this up...\n",
    "pp_keyword_list.remove('-')\n",
    "pp_keyword_list.remove('school')\n",
    "pp_keyword_list.remove('academy')\n",
    "pp_keyword_list.remove('charter')\n",
    "pp_keyword_list.remove('high')\n",
    "pp_keyword_list.remove('jhs')\n",
    "pp_keyword_list.remove('middle')\n",
    "pp_keyword_list.remove('secondary')\n",
    "pp_keyword_list.remove('senior')\n",
    "pp_keyword_list.remove('h')\n",
    "pp_keyword_list.remove('j')\n",
    "pp_keyword_list.remove('e')\n",
    "#pp_keyword_list.remove('c')\n",
    "#pp_keyword_list.remove('f')\n",
    "#pp_keyword_list.remove('w')\n",
    "#pp_keyword_list.remove('b')\n",
    "#pp_keyword_list.remove('r')\n",
    "#pp_keyword_list.remove('l')\n",
    "pp_keyword_list.remove('st')\n",
    "#pp_keyword_list.remove('st.')\n",
    "filtered_pp_keywords = [word for word in pp_keyword_list if word not in stopwords.words('english')]\n",
    "\n",
    "# back to uppercase to match original dataset\n",
    "filtered_pp_keywords = [x.upper() for x in filtered_pp_keywords]\n",
    "\n",
    "write_keyword_list_to_txt(filtered_pp_keywords, 'park_playground_keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEIGHBORHOODS\n",
    "\n",
    "# neighborhood filelist (nh)\n",
    "nh_list = sc.textFile(\"neighborhood_filelist\")\n",
    "nh_list = nh_list.flatMap(lambda x: x.split(\",\")).collect()\n",
    "nh_list = [x.strip('\"') for x in nh_list]\n",
    "nh_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in nh_list]\n",
    "nh_list = [re.sub(\" \" \"\", \"\", item)for item in nh_list]\n",
    "\n",
    "# generate keyword list for city agencies\n",
    "nh_current_list = []\n",
    "nh_keyword_list = []\n",
    "for filename in nh_list: # first. change to map later\n",
    "    nh_df = sqlContext.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "    nh_df_clean = nh_df.select(\"val\", f.regexp_replace(f.col(\"val\"), \"[\\-$#&,]\", \"\").alias(\"clean_word\"))\n",
    "\n",
    "    # split words in each row and create new df with one word per row, and count\n",
    "    nh_df_split_words = nh_df_clean.withColumn('word', f.explode(f.split(f.lower(f.col('clean_word')), ' ')))\\\n",
    "        .groupBy('word')\\\n",
    "        .count()\\\n",
    "        .sort('count', ascending=False)\\\n",
    "        .filter(\"word != ''\")\n",
    "\n",
    "    #ca_df_new = ca_df_split_words.withColumn(\"word\", array(ca_df_split_words[\"word\"]))\n",
    "       \n",
    "    nh_current_list = [row['word'] for row in nh_df_split_words.take(20)]\n",
    "    nh_keyword_list = list(set(nh_keyword_list).union(set(nh_current_list)))\n",
    "\n",
    "# remove stopwords (sw list is lowercase)\n",
    "nh_keyword_list.remove('st')\n",
    "nh_keyword_list.remove('avenue')\n",
    "#nh_keyword_list.remove('')\n",
    "filtered_nh_keywords = [word for word in nh_keyword_list if word not in stopwords.words('english')]\n",
    "\n",
    "# back to uppercase to match \n",
    "filtered_nh_keywords = [x.upper() for x in filtered_nh_keywords]\n",
    "\n",
    "write_keyword_list_to_txt(filtered_nh_keywords, 'neighborhood_keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5uac-w243.PREM_TYP_DESC.txt.gz\n",
      "qgea-i56i.PREM_TYP_DESC.txt.gz\n"
     ]
    }
   ],
   "source": [
    "# LOCATION TYPE\n",
    "\n",
    "# location type filelist (lt)\n",
    "lt_list = sc.textFile(\"location_type_filelist\")\n",
    "lt_list = lt_list.flatMap(lambda x: x.split(\",\")).collect()\n",
    "lt_list = [x.strip('\"') for x in lt_list]\n",
    "lt_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in lt_list]\n",
    "lt_list = [re.sub(\" \" \"\", \"\", item)for item in lt_list]\n",
    "\n",
    "# generate keyword list for city agencies\n",
    "lt_current_list = []\n",
    "lt_keyword_list = []\n",
    "for filename in lt_list: # first. change to map later\n",
    "    print(filename)\n",
    "    lt_df = sqlContext.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "    lt_df_clean = lt_df.select(\"val\", f.regexp_replace(f.col(\"val\"), \"[\\-$#&,]\", \"\").alias(\"clean_word\"))\n",
    "\n",
    "    # split words in each row and create new df with one word per row, and count\n",
    "    lt_df_split_words = lt_df_clean.withColumn('word', f.explode(f.split(f.lower(f.col('clean_word')), ' ')))\\\n",
    "        .groupBy('word')\\\n",
    "        .count()\\\n",
    "        .sort('count', ascending=False)\\\n",
    "        .filter(\"word != ''\")\n",
    "\n",
    "    #ca_df_new = ca_df_split_words.withColumn(\"word\", array(ca_df_split_words[\"word\"]))\n",
    "       \n",
    "    lt_current_list = [row['word'] for row in lt_df_split_words.take(100)]\n",
    "    lt_keyword_list = list(set(lt_keyword_list).union(set(lt_current_list)))\n",
    "\n",
    "# remove stopwords (sw list is lowercase)\n",
    "#lt_keyword_list.remove('street')\n",
    "lt_keyword_list.remove('nyc')\n",
    "filtered_lt_keywords = [word for word in lt_keyword_list if word not in stopwords.words('english')]\n",
    "\n",
    "# back to uppercase to match \n",
    "filtered_lt_keywords = [x.upper() for x in filtered_lt_keywords]\n",
    "\n",
    "write_keyword_list_to_txt(filtered_lt_keywords, 'location_type_keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4y63-yw9e.SCHOOL_NAME.txt.gz\n",
      "gk83-aa6y.SCHOOL_NAME.txt.gz\n",
      "xne4-4v8f.SCHOOL.txt.gz\n",
      "sqcr-6mww.School_Name.txt.gz\n",
      "5e7x-8jy6.School_Name.txt.gz\n",
      "3rfa-3xsf.School_Name.txt.gz\n",
      "wks3-66bn.School_Name.txt.gz\n",
      "hy4q-igkk.School_Name.txt.gz\n",
      "x3kb-2vbv.School_Name.txt.gz\n",
      "imfa-v5pv.School_Name.txt.gz\n",
      "jtus-srrj.School_Name.txt.gz\n",
      "jzt2-2f7h.School_Name.txt.gz\n"
     ]
    }
   ],
   "source": [
    "# SCHOOL NAME\n",
    "\n",
    "# school name type filelist (sn)\n",
    "sn_list = sc.textFile(\"school_name_type_filelist\")\n",
    "sn_list = sn_list.flatMap(lambda x: x.split(\",\")).collect()\n",
    "sn_list = [x.strip('\"') for x in sn_list]\n",
    "sn_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in sn_list]\n",
    "sn_list = [re.sub(\" \" \"\", \"\", item)for item in sn_list]\n",
    "\n",
    "# generate keyword list for city agencies\n",
    "sn_current_list = []\n",
    "sn_keyword_list = []\n",
    "for filename in sn_list: # first. change to map later\n",
    "    print(filename)\n",
    "    sn_df = sqlContext.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "    sn_df_clean = sn_df.select(\"val\", f.regexp_replace(f.col(\"val\"), \"[\\-$#&,]\", \"\").alias(\"clean_word\"))\n",
    "\n",
    "    # split words in each row and create new df with one word per row, and count\n",
    "    sn_df_split_words = sn_df_clean.withColumn('word', f.explode(f.split(f.lower(f.col('clean_word')), ' ')))\\\n",
    "        .groupBy('word')\\\n",
    "        .count()\\\n",
    "        .sort('count', ascending=False)\\\n",
    "        .filter(\"word != ''\")\n",
    "\n",
    "    #ca_df_new = ca_df_split_words.withColumn(\"word\", array(ca_df_split_words[\"word\"]))\n",
    "       \n",
    "    sn_current_list = [row['word'] for row in sn_df_split_words.take(20)]\n",
    "    sn_keyword_list = list(set(sn_keyword_list).union(set(sn_current_list)))\n",
    "\n",
    "# remove stopwords (sw list is lowercase)\n",
    "#lt_keyword_list.remove('street')\n",
    "#sn_keyword_list.remove('nyc')\n",
    "filtered_sn_keywords = [word for word in sn_keyword_list if word not in stopwords.words('english')]\n",
    "\n",
    "# back to uppercase to match \n",
    "filtered_sn_keywords = [x.upper() for x in filtered_sn_keywords]\n",
    "\n",
    "write_keyword_list_to_txt(filtered_sn_keywords, 'school_name_type_keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d3ge-anaz.CORE_COURSE__MS_CORE_and_9_12_ONLY_.txt.gz\n",
      "6wcu-cfa3.CORE_COURSE__MS_CORE_and_9_12_ONLY_.txt.gz\n",
      "ajgi-hpq9.CORE_SUBJECT___MS_CORE_and__09_12_ONLY_.txt.gz\n",
      "ub9e-s7ai.CORE_SUBJECT___MS_CORE_and__09_12_ONLY_.txt.gz\n",
      "pgtq-ht5f.CORE_SUBJECT___MS_CORE_and__9_12_ONLY_.txt.gz\n",
      "ytjm-yias.CORE_SUBJECT___MS_CORE_and__09_12_ONLY_.txt.gz\n",
      "5nz7-hh6t.CORE_SUBJECT___MS_CORE_and__09_12_ONLY_.txt.gz\n",
      "gez6-674h.CORE_SUBJECT___MS_CORE_and__9_12_ONLY_.txt.gz\n",
      "f7qh-bcr5.CORE_SUBJECT___MS_CORE_and__9_12_ONLY_.txt.gz\n",
      "7yds-6i8e.CORE_SUBJECT__MS_CORE_and_9_12_ONLY_.txt.gz\n",
      "kz72-dump.CORE_SUBJECT___MS_CORE_and__09_12_ONLY_.txt.gz\n",
      "sybh-s59s.CORE_SUBJECT___MS_CORE_and__9_12_ONLY_.txt.gz\n",
      "6ypq-ih9a.CORE_SUBJECT___MS_CORE_and__09_12_ONLY_.txt.gz\n",
      "3aka-ggej.CORE_SUBJECT___MS_CORE_and__09_12_ONLY_.txt.gz\n",
      "i8ys-e4pm.CORE_COURSE_9_12_ONLY_.txt.gz\n",
      "8i43-kna8.CORE_SUBJECT.txt.gz\n"
     ]
    }
   ],
   "source": [
    "# SCHOOL SUBJECT\n",
    "\n",
    "# school subject type filelist (ss)\n",
    "ss_list = sc.textFile(\"school_subject_type_filelist\")\n",
    "ss_list = ss_list.flatMap(lambda x: x.split(\",\")).collect()\n",
    "ss_list = [x.strip('\"') for x in ss_list]\n",
    "ss_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in ss_list]\n",
    "ss_list = [re.sub(\" \" \"\", \"\", item)for item in ss_list]\n",
    "\n",
    "# generate keyword list for city agencies\n",
    "ss_current_list = []\n",
    "ss_keyword_list = []\n",
    "for filename in ss_list: # first. change to map later\n",
    "    print(filename)\n",
    "    ss_df = sqlContext.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "    ss_df_clean = ss_df.select(\"val\", f.regexp_replace(f.col(\"val\"), \"[\\-$#&,]\", \"\").alias(\"clean_word\"))\n",
    "\n",
    "    # split words in each row and create new df with one word per row, and count\n",
    "    ss_df_split_words = ss_df_clean.withColumn('word', f.explode(f.split(f.lower(f.col('clean_word')), ' ')))\\\n",
    "        .groupBy('word')\\\n",
    "        .count()\\\n",
    "        .sort('count', ascending=False)\\\n",
    "        .filter(\"word != ''\")\n",
    "\n",
    "    #ca_df_new = ca_df_split_words.withColumn(\"word\", array(ca_df_split_words[\"word\"]))\n",
    "       \n",
    "    ss_current_list = [row['word'] for row in ss_df_split_words.take(50)]\n",
    "    ss_keyword_list = list(set(ss_keyword_list).union(set(ss_current_list)))\n",
    "\n",
    "# remove stopwords (sw list is lowercase)\n",
    "#lt_keyword_list.remove('street')\n",
    "ss_keyword_list.remove('11')\n",
    "ss_keyword_list.remove('10')\n",
    "ss_keyword_list.remove('b')\n",
    "ss_keyword_list.remove('9')\n",
    "ss_keyword_list.remove('12')\n",
    "filtered_ss_keywords = [word for word in ss_keyword_list if word not in stopwords.words('english')]\n",
    "\n",
    "# back to uppercase to match \n",
    "filtered_ss_keywords = [x.upper() for x in filtered_ss_keywords]\n",
    "\n",
    "write_keyword_list_to_txt(filtered_ss_keywords, 'school_subject_type_keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GEOMETRY',\n",
       " 'HISTORY',\n",
       " 'ECONOMICS',\n",
       " 'SECTIONS',\n",
       " 'SOCIAL',\n",
       " 'US',\n",
       " 'STUDIES',\n",
       " 'CHEMISTRY',\n",
       " 'EARTH',\n",
       " 'TEAM',\n",
       " 'MATH',\n",
       " 'GLOBAL',\n",
       " 'GOVERNMENT',\n",
       " 'LIVING',\n",
       " 'SCIENCE',\n",
       " 'MATCHED',\n",
       " 'ENGLISH',\n",
       " 'PHYSICS',\n",
       " 'TEACHING',\n",
       " 'ENVIRONMENT',\n",
       " 'ALGEBRA',\n",
       " 'ASSUMED']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ss_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# other option: do this instead of above if you want to mine frequent itemsets later\n",
    "# (probably useful for strings with at least a few words)\n",
    "#pp_df_split_words = pp_df.withColumn('word', f.explode(f.split(f.col('val'), ' ')))\\\n",
    "#    .select('word')\n",
    "#pp_df.show()\n",
    "#pp_df_new = pp_df.select(col(\"val\"), split(col(\"val\"), \" \\s*\").cast(ArrayType(StringType())).alias(\"word\"))\n",
    "pp_df_new = pp_df.withColumn(\"word\", array(pp_df[\"val\"]))\n",
    "\n",
    "\n",
    "fpGrowth = FPGrowth(itemsCol=\"word\", minSupport=0.00, minConfidence=0.0)\n",
    "model = fpGrowth.fit(pp_df_new)\n",
    "model.freqItemsets.show()\n",
    "#pp_df_new.show()\n",
    "#model.associationRules.show()\n",
    "#model.transform(pp_df_new).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# other option: do this instead of above if you want to mine frequent itemsets later\n",
    "# (probably useful for strings with at least a few words)\n",
    "#df_split_words = df.withColumn('word', f.explode(f.split(f.col('val'), ' ')))\\\n",
    "#    .select('word')\n",
    "#df_new = df_split_words.withColumn(\"word\", array(df_split_words[\"word\"]))\n",
    "#fpGrowth = FPGrowth(itemsCol=\"word\", minSupport=0.1, minConfidence=0.3)\n",
    "#model = fpGrowth.fit(df_new)\n",
    "#model.freqItemsets.show()\n",
    "#df_new.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

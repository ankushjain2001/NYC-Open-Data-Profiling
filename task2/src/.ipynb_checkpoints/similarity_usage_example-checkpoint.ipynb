{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- NOTES -------------------------------------------------------------------\n",
    "# 1. Update the datasets, dataList\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pyspark\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import DoubleType\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.sql.functions import udf, unix_timestamp, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Input & output directories\n",
    "\n",
    "    #file_list = \"../cluster3.txt\"\n",
    "    file_list = \"/home/ted/school/big_data/project/big_data_course_project/cluster3.txt\"\n",
    "\n",
    "    # use these if running locally (Jupyter, etc)\n",
    "    inputDirectory = \"../raw_data/\"\n",
    "    outputDirectory = \"../output_data/\"#sys.argv[2]\n",
    "    \n",
    "    # use these if running on DUMBO\n",
    "    #inputDirectory = \"/user/hm74/NYCColumns/\"#sys.argv[1]\n",
    "    #outputDirectory = \"/user/\" + this_user + \"/project/task1/\"#sys.argv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Setting spark context and \n",
    "    sc = SparkContext()\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"project_task1\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
    "\n",
    "    # Current user path\n",
    "    env_var = os.environ\n",
    "    this_user = env_var['USER']\n",
    "\n",
    "    # Input & output directories. Use these if running on DUMBO\n",
    "    #inputDirectory = \"/user/hm74/NYCOpenData/\"#sys.argv[1]\n",
    "    #outputDirectory = \"/user/\" + this_user + \"/project/task1/\"#sys.argv[2]\n",
    "    \n",
    "    # use these if running locally (Jupyter, etc)\n",
    "    inputDirectory = \"../raw_data/\"\n",
    "    outputDirectory = \"../output_data/\"#sys.argv[2]\n",
    "\n",
    "    # Output JSON Semantic Schema\n",
    "    semanticSchema = {\n",
    "        \"semantic_type\": \"\",\n",
    "        \"count\": 0\n",
    "    }\n",
    "\n",
    "    # Define of different types regex dict:\n",
    "    expr_dic = {\"Street\": \"ROAD|STREET|PLACE|DRIVE|BLVD|%ST%|%RD%|DR|AVENUE\",\n",
    "                \"Website\" : \"WWW.|.COM|HTTP://\",\n",
    "                \"BuildingCode\" : \"([A-Z])\\d\\-\",\n",
    "                \"PhoneBumber\":\"\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d|\\(\\d\\d\\d\\)\\d\\d\\d\\d\\d\\d\\d|\\d\\d\\d\\-\\d\\d\\d\\-\\d\\d\\d\\d\",\n",
    "                \"ZipCode\":\"\\d\\d\\d\\d\\d|\\d\\d\\d\\d\\d\\-\\d\\d\\d|\\d\\d\\d\\d\\d\\d\\d\\d\",\n",
    "                \"Lat_Lon\" : \"\\([-+]?[0-9]+\\.[0-9]+\\,\\s*[-+]?[0-9]+\\.[0-9]+\\)\",\n",
    "                \"SchoolName\" : \"SCHOOL|ACADEMY|HS|ACAD|I.S.|IS|M.S.|P.S|PS|YABC\",\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset =========== :  1 - faiq-9dfq.Vehicle_Color.txt.gz\n",
      "After Scaling :\n",
      "+-----+-------+------------+\n",
      "|  val|  count|count_Scaled|\n",
      "+-----+-------+------------+\n",
      "|   WH|2204226|         1.0|\n",
      "|   GY|1930859|       0.876|\n",
      "|   BK|1714273|       0.778|\n",
      "|WHITE|1243122|       0.564|\n",
      "|   BL| 670601|       0.304|\n",
      "|BLACK| 524911|       0.238|\n",
      "|   RD| 430013|       0.195|\n",
      "| GREY| 347729|       0.158|\n",
      "|BROWN| 314730|       0.143|\n",
      "|SILVE| 223641|       0.101|\n",
      "| BLUE| 207777|       0.094|\n",
      "|  RED| 185461|       0.084|\n",
      "|   GR| 181040|       0.082|\n",
      "|   TN| 118904|       0.054|\n",
      "|   YW|  94288|       0.043|\n",
      "|OTHER|  91116|       0.041|\n",
      "|  BLK|  90186|       0.041|\n",
      "|   BR|  88584|        0.04|\n",
      "|GREEN|  73344|       0.033|\n",
      "|   GL|  52874|       0.024|\n",
      "+-----+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # Importing cluster3 format it and put it into a list\n",
    "    raw_data = sc.textFile(\"cluster3.txt\")\n",
    "    raw_list = raw_data.flatMap(lambda x: x.split(\",\")).collect()\n",
    "    raw_list = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in raw_list]\n",
    "    raw_list = [re.sub(\" \" \"\", \"\", item)for item in raw_list]\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Iteration over dataframes begins bu using dataframe file names\n",
    "    processCount = 1\n",
    "\n",
    "    # Create schema for raw data before reading into df \n",
    "    customSchema = StructType([\n",
    "               StructField(\"val\", StringType(), True),\n",
    "               StructField(\"count\", IntegerType(), True)])\n",
    "    #Testing first 50 files\n",
    "    for filename in raw_list[2:3]:\n",
    "\n",
    "        print(\"Processing Dataset =========== : \", str(processCount) + ' - ' +filename)\n",
    "        df = sqlContext.read.format(\"csv\").option(\"header\",\n",
    "        \"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \n",
    "        \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "\n",
    "        # Count all val in df with count \n",
    "        count_all = df.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        \n",
    "        \n",
    "        # note: the following scaler process is taken from:\n",
    "        # https://stackoverflow.com/questions/40337744/scalenormalise-a-column-in-spark-dataframe-pyspark\n",
    "        # UDF for converting column type from vector to double type\n",
    "        unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "       \n",
    "        for i in [\"count\"]:\n",
    "            # VectorAssembler Transformation - Converting column to vector type\n",
    "            assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "            # MinMaxScaler Transformation\n",
    "            scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "\n",
    "            # Pipeline of VectorAssembler and MinMaxScaler\n",
    "            pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "            # Fitting pipeline on dataframe\n",
    "            df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "\n",
    "        print(\"After Scaling :\")\n",
    "        df.sort(col(\"count\").desc()).show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\"\"\" END HERE\"\"\"\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- NOTES -------------------------------------------------------------------\n",
    "# 1. Update the datasets, dataList\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pyspark\n",
    "from ast import literal_eval\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.sql.functions import udf, unix_timestamp, col ,length\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, FloatType, DateType, TimestampType\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline \n",
    "from collections import Counter\n",
    "#import spacy\n",
    "#from spacy import displacy\n",
    "#import en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# --- Function Definitions Begin ----------------------------------------------\n",
    "\n",
    "# Function to find mean and stdv for all files\n",
    "def mean_stdv(df):\n",
    "    unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "    for i in [\"count\"]:\n",
    "        assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "        scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "        pipeline = Pipeline(stages=[assembler, scaler])\n",
    "        df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "        df_stats = df.select(_mean(col('count_Scaled')).alias('mean'),_stddev(col('count_Scaled')).alias('std')).collect()\n",
    "        mean = df_stats[0]['mean']\n",
    "        std = df_stats[0]['std']\n",
    "        return df_stats \n",
    "\n",
    "# Function to sum all count of values for all files\n",
    "def count_all_values(df):\n",
    "    res = df.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "    return res\n",
    "\n",
    "# Regex function to check website type\n",
    "def re_find_website(df,count_all,found_type):\n",
    "    web_re_rexpr = \"WWW\\.|\\.COM|HTTP\\:\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(web_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"website\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function to check zip type\n",
    "def re_find_zipCode(df,count_all,found_type):\n",
    "    zip_re_rexpr = \"^\\d{5}?$|^\\d{5}?-\\d\\d\\d$|^\\d{8}?$\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(zip_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"zip_code\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function to check buildingCode type\n",
    "def re_find_buildingCode(df,count_all,found_type):\n",
    "    bc_re_rexpr = \"([A-Z])\\d\\-\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(bc_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"building_classification\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0 \n",
    "\n",
    "# Regex function to check phone number type\n",
    "def re_find_phoneNum(df,count_all,found_type):\n",
    "    phone_re_rexpr = \"^\\d{10}?$|^\\(\\d\\d\\d\\)\\d\\d\\d\\d\\d\\d\\d$|^\\d\\d\\d\\-\\d\\d\\d\\-\\d\\d\\d\\d$\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(phone_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"phone_number\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function to check lat_lon type\n",
    "def re_find_lat_lon(df,count_all,found_type):\n",
    "    ll_re_rexpr = \"\\([-+]?[0-9]+\\.[0-9]+\\,\\s*[-+]?[0-9]+\\.[0-9]+\\)\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(ll_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"lat_lon_cord\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function to check street_addrees type\n",
    "def re_find_street_address(df,count_all,col_length,found_type):\n",
    "    st_re_rexpr = \"\\sROAD|\\sSTREET|\\sPLACE|\\sDRIVE|\\sBLVD|\\sST|\\sRD|\\sDR|\\sAVENUE|\\sAVE\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(st_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.8): \n",
    "            if (col_length >= 15):\n",
    "                found_type = found_type + [\"address\"]\n",
    "            elif (col_length < 15):\n",
    "                found_type = found_type + [\"street\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function to check school name type\n",
    "def re_find_school(df,count_all,found_type):\n",
    "    school_re_rexpr = \"\\sSCHOOL|\\sACADEMY|HS\\s|ACAD|I.S.\\s|IS\\s|M.S.\\s|P.S\\s|PS\\s|ACADEMY\\s\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(school_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.5): \n",
    "            found_type = found_type + [\"school_name\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function for checking house number \n",
    "def re_find_houseNo(df,count_all,found_type):\n",
    "    houseNo_re_rexpr = \"^\\d{2}?$|^\\d{3}?$|^\\d{4}?$\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(houseNo_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"house number\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function for checking school subject\n",
    "def re_find_school_subject(df,count_all,found_type):\n",
    "    school_subj_re_rexpr = \"^ENGLISH$|^ENGLISH\\s[0-9]?$|^MATH\\s[A-Z$]|^MATH$|^SCIENCE$|^SOCIAL\\sSTUDIES$|^ALGEBRA\\s[A-Z]$|\\\n",
    "                            ^CHEMISTRY$|^ASSUMED\\sTEAM\\sTEACHING$|^EARTH\\sSCIENCE$|^GEOMETRY$|^ECONOMICS$|^GLOBAL HISTORY$|\\\n",
    "                            ^GLOBAL\\sHISTORY[A-Z]$|^LIVING ENVIRONMENT$|^PHYSICS$|^US\\sGOVERNMENT$|^US\\sGOVERNMENT$|^US\\sGOVERNMENT\\s&|\\\n",
    "                            ^US\\SHISTORY$|^GLOBAL HISTORY\\s[0-9]?$\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(school_subj_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.5): \n",
    "            found_type = found_type + [\"school subject\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function for checking school level \n",
    "def re_find_schoolLevel(df,count_all,found_type):\n",
    "    schlvl_re_rexpr = \"^[K]\\-\\d?$|^HIGH SCHOOL$|^ELEMENTARY$|^ELEMENTARY SCHOOL$|^MIDDLE SCHOOL$|^TRANSFER\\sSCHOOL$|^MIDDLE$|^HIGH\\sSCHOOL\\sTRANSFERL$|^YABC$|^[K]\\-[0-9]{2}$\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(schlvl_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"school level\"]\n",
    "        return res, found_type, count_filtered\n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# --- Functions FOR NLP Starts HERE -------------------------------------------\n",
    "def nlp_find_person(df,count_all,found_type):\n",
    "    #Your Code HERE: \n",
    "    #Use count_all for percentage calculation\n",
    "    #Please return two values: (1)percentage of such type in this col AND (2)the type found for this column\n",
    "    #if found:\n",
    "#         found_type = found_type + [\"person\"]\n",
    "    #if not found:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_business_name(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_vehicle_type(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_color(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_car_make(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_car_model(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_neighborhood(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_borough(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_city(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "# --- Function FOR NLP End ------------------------------------------------\n",
    "\n",
    "# --- Functions FOR LIST COMPARISON Starts HERE -------------------------------\n",
    "def list_find_school_subject(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(ss_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.4): \n",
    "            found_type = found_type + [\"school subject\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "def list_find_business_name(df,count_all,found_type):\n",
    "    #Your Code HERE: \n",
    "    return 0, found_type, 0\n",
    "\n",
    "def list_find_neighborhood(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(nh_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.4): \n",
    "            found_type = found_type + [\"neighborhood\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "def list_find_area_of_study(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(aos_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.3): \n",
    "            found_type = found_type + [\"area of study\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "    \n",
    "\n",
    "def list_find_agency(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(ca_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.4): \n",
    "            found_type = found_type + [\"city agency\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "def list_find_location_type(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(lt_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.4): \n",
    "            found_type = found_type + [\"city agency\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "def list_find_parks_playgrounds(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(pp_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.4): \n",
    "            found_type = found_type + [\"parks and playgrounds\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "def import_keyword_list(inputDir):\n",
    "    klist = sc.textFile(inputDir)\n",
    "    klist = klist.flatMap(lambda x: x.split(\",\")).collect()\n",
    "    klist = [x.strip('\"') for x in klist]\n",
    "    klist = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in klist]\n",
    "    klist = [re.sub(\" \" \"\", \"\", item)for item in klist]\n",
    "    return(klist)\n",
    "\n",
    "\n",
    "# --- Function Definitions End ------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# --- MAIN --------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setting spark context and \n",
    "    sc = SparkContext()\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"project_task2\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    # Current user path\n",
    "    env_var = os.environ\n",
    "    this_user = env_var['USER']\n",
    "\n",
    "    # Input & output directories\n",
    "    #inputDirectory = \"/user/hm74/NYCColumns/\"#sys.argv[1]\n",
    "    #outputDirectory = \"/user/\" + this_user + \"/project/task2/\"#sys.argv[2]\n",
    "    inputDirectory = \"/home/ted/school/big_data/project/big_data_course_project/task2/raw_data/\"\n",
    "    inputFileClusters = \"/home/ted/school/big_data/project/big_data_course_project/task2/resources/filename_clusters.json\"\n",
    "    input_pp_keywords = \"park_playground_keywords\"\n",
    "    input_aos_keywords = \"area_of_study_keywords\"\n",
    "    input_ca_keywords = \"city_agency_keywords\"\n",
    "    input_ss_keywords = \"school_subject_keywords\"\n",
    "    input_sn_keywords = \"school_name_keywords\"\n",
    "    input_lt_keywords = \"location_type_keywords\"\n",
    "    input_nh_keywords = \"neighborhood_keywords\"\n",
    "    \n",
    "    pp_keywords = import_keyword_list(input_pp_keywords) # parks & playgrounds\n",
    "    aos_keywords = import_keyword_list(input_aos_keywords) # area of study\n",
    "    ca_keywords = import_keyword_list(input_ca_keywords) # city agency\n",
    "    ss_keywords = import_keyword_list(input_ss_keywords) # school subject\n",
    "    sn_keywords = import_keyword_list(input_sn_keywords) # school name\n",
    "    lt_keywords = import_keyword_list(input_lt_keywords) # location type\n",
    "    nh_keywords = import_keyword_list(input_nh_keywords) # neighborhood\n",
    "    \n",
    "    \n",
    "    # Output JSON Semantic Schema\n",
    "    jsonSchema = {\n",
    "        \"column_name\": \"\",\n",
    "        \"semantic_type\": [],\n",
    "        \"count\": 0\n",
    "    }\n",
    "\n",
    "    # Inner semantic schema \n",
    "    semanticSchema = {\n",
    "        \"semantic_type\": \"\",\n",
    "        \"label\": \"\",\n",
    "        \"count\": 0 \n",
    "    }\n",
    "\n",
    "    # Importing cluster3 format it and put it into a list\n",
    "    #raw_data = sc.textFile(\"/user/aj2885/Project_Resource/cluster3_labels.tsv\")\n",
    "    raw_data = sc.textFile(\"true_labels.tsv\")\n",
    "    raw_list = raw_data.map(lambda x: x.split(\"\\t\")).collect()\n",
    "\n",
    "    # Iteration over dataframes begins bu using dataframe file names\n",
    "    processCount = 1\n",
    "\n",
    "    # Create schema for raw data before reading into df \n",
    "    customSchema = StructType([\n",
    "                StructField(\"val\", StringType(), True),\n",
    "                StructField(\"count\", IntegerType(), True)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset =========== :  11 - vw9i-7mzq.interest3.txt.gz\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e7d0fb2b6a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#added col_length which is the average length of the col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdf_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avg_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mcol_length\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "    #Testing first 10 files\n",
    "    for filerow in raw_list[0:10]:\n",
    "        filename = filerow[0]\n",
    "        labels = literal_eval(filerow[1])\n",
    "        print(\"Processing Dataset =========== : \", str(processCount) + ' - ' +filename)\n",
    "        # Read file to dataset and apply all regex functions\n",
    "        found_type = []\n",
    "        fileinfo = []\n",
    "        regex_res = []\n",
    "        df = sqlContext.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "        df_stats = mean_stdv(df)\n",
    "        mean = df_stats[0]['mean']\n",
    "        std = df_stats[0]['std']\n",
    "        count_all = count_all_values(df)\n",
    "\n",
    "        #added col_length which is the average length of the col\n",
    "        df_length = df.select(_mean(length(col(\"val\"))).alias('avg_length'))\n",
    "        col_length= df_length.collect()[0][0]\n",
    "\n",
    "        percentage_website, found_type, type_count_web = re_find_website(df,count_all,found_type)\n",
    "        percentage_zip, found_type, type_count_zip = re_find_zipCode(df,count_all,found_type)\n",
    "        percentage_buildingCode, found_type,type_count_building = re_find_buildingCode(df,count_all,found_type)\n",
    "        percentage_phoneNum, found_type, type_count_phone = re_find_phoneNum(df,count_all,found_type)\n",
    "        percentage_lat_lon, found_type, type_count_lat_lon = re_find_lat_lon(df,count_all,found_type)\n",
    "        percentage_add_st, found_type, type_count_add_st = re_find_street_address(df,count_all,col_length,found_type)\n",
    "        percentage_school_name, found_type, type_count_school_name= re_find_school(df,count_all,found_type)\n",
    "        percentage_house_no, found_type ,type_count_house_no= re_find_houseNo(df,count_all,found_type)\n",
    "        percentage_school_lvl, found_type, type_count_school_lvl= re_find_schoolLevel(df,count_all,found_type)\n",
    "        percentage_school_subject, found_type, type_count_school_subject= re_find_school_subject(df,count_all,found_type)\n",
    "        \n",
    "        # moved this block up here -ted\n",
    "        percentage_area_of_study, found_type, type_count_area_of_study = list_find_area_of_study(df,count_all,found_type)\n",
    "        percentage_school_subject, found_type, type_count_school_subject= list_find_school_subject(df,count_all,found_type)\n",
    "        percentage_agency, found_type, type_count_agency= list_find_agency(df,count_all,found_type)\n",
    "        percentage_location, found_type, type_count_location= list_find_location_type(df,count_all,found_type)\n",
    "        percentage_neighborhood, found_type, type_count_neighborhood= list_find_neighborhood(df,count_all,found_type)\n",
    "        percentage_parks_playgrounds, found_type, type_count_parks_playgrounds = list_find_parks_playgrounds(df,count_all,found_type)\n",
    "        \n",
    "        type_count = type_count_web + type_count_zip + type_count_building + type_count_phone + \\\n",
    "                    type_count_lat_lon + type_count_add_st + type_count_school_name + \\\n",
    "                    type_count_house_no + type_count_school_lvl + type_count_school_subject + \\\n",
    "                    type_count_area_of_study + type_count_neighborhood + type_count_agency + \\\n",
    "                    type_count_location + type_count_parks_playgrounds\n",
    "        \n",
    "        #give a default value for all other precentages \n",
    "        percentage_person = 0\n",
    "        percentage_business_name = 0\n",
    "        percentage_vehicle_type = 0\n",
    "        percentage_color = 0\n",
    "        percentage_car_make = 0\n",
    "        percentage_car_model = 0\n",
    "        #percentage_neighborhood = 0\n",
    "        percentage_borough= 0 \n",
    "        percentage_city = 0\n",
    "        #percentage_area_of_study = 0\n",
    "        #percentage_location = 0\n",
    "        #percentage_agency = 0\n",
    "        #percentage_parks_playgrounds = 0\n",
    "\n",
    "        #STEP TWO: NLP LABEL AND LIST CHECK\n",
    "        # if not found_type:\n",
    "        #     #ANKUSH PART: NLP CHECK TYPES\n",
    "        #     percentage_person, found_type, type_count_person = nlp_find_person(df,count_all,found_type)\n",
    "        #     percentage_business_name, found_type, type_count_business = nlp_find_business_name(df,count_all,found_type)\n",
    "        #     percentage_vehicle_type, found_type, type_count_vehicle_type = nlp_find_vehicle_type(df,count_all,found_type)\n",
    "        #     percentage_color, found_type, type_count_color = nlp_find_color(df,count_all,found_type)\n",
    "        #     percentage_car_make, found_type, type_count_car_make = nlp_find_car_make(df,count_all,found_type)\n",
    "        #     percentage_car_model, found_type, type_count_car_model = nlp_find_car_model(df,count_all,found_type)\n",
    "        #     percentage_neighborhood, found_type, type_count_neighborhood = nlp_find_neighborhood(df,count_all,found_type)\n",
    "        #     percentage_borough, found_type, type_count_borough = nlp_find_borough(df,count_all,found_type)\n",
    "        #     percentage_city, found_type, type_count_city = nlp_find_city(df,count_all,found_type)\n",
    "        \n",
    "        #     #TED PART: LIST or SIMILARITY CHECK TYPEs\n",
    "        #   percentage_school_subject, found_type, type_count_school_subject= list_find_school_subject(df,count_all,found_type)\n",
    "        #     percentage_business_name, found_type, type_count_business= list_find_business_name(df,count_all,found_type)\n",
    "        #   percentage_neighborhood, found_type, type_count_neighborhood= list_find_neighborhood(df,count_all,found_type)\n",
    "        #   percentage_area_of_study, found_type, type_count_area_of_study = list_find_area_of_study(df,count_all,found_type)\n",
    "        #   percentage_agency, found_type, type_count_agency= list_find_agency(df,count_all,found_type)\n",
    "        #   percentage_location, found_type, type_count_location= list_find_location_type(df,count_all,found_type)\n",
    "        #   percentage_parks_playgrounds, type_count_location_parks_playgrounds = list_find_parks_playgrounds(df,count_all,found_type\n",
    "        # !!! NOTE: Please remeber to add type_count_XXX back to type_count in LINE 347\n",
    "        fileinfo.extend([filename,mean,std,count_all,col_length, percentage_website, percentage_zip,percentage_buildingCode,percentage_phoneNum,percentage_lat_lon,percentage_add_st,percentage_school_name,percentage_house_no,percentage_school_lvl,percentage_person,percentage_school_subject,percentage_vehicle_type, percentage_color,percentage_car_make,percentage_car_model,percentage_neighborhood,percentage_borough,percentage_city,percentage_business_name,percentage_area_of_study,percentage_location,percentage_parks_playgrounds,found_type, type_count])\n",
    "        regex_res.append(fileinfo)\n",
    "        print(regex_res)\n",
    "        # USE ME to export the JSON for current dataset\n",
    "        print(\"Saving Dataset =============== : \", str(processCount) + ' - ' +filename)\n",
    "        processCount += 1\n",
    "        #outJSON = deepcopy(jsonSchema)\n",
    "        #outJSON[\"column_name\"] = filename\n",
    "        #outJSON[\"semantic_type\"] = found_type\n",
    "        #outJSON[\"count\"] = type_count\n",
    "        #outJSON = sc.parallelize([json.dumps(outJSON)])\n",
    "        #outJSON.saveAsTextFile(outputDirectory + filename + '/task2.json')\n",
    "\n",
    "\n",
    "\n",
    "    # Output regex function result \n",
    "    rdd = sc.parallelize(regex_res)\n",
    "    row_rdd = rdd.map(lambda x: Row(x))\n",
    "    df = row_rdd.toDF()\n",
    "    df = df.select(col('_1').alias('coln'))\n",
    "    length = len(df.select('coln').take(1)[0][0])\n",
    "    df = df.select([df.coln[i] for i in range(length)])\n",
    "    df = df.select(col('coln[0]').alias('filename'),col('coln[1]').alias('mean'),col('coln[2]').alias('stdv'),\n",
    "               col('coln[3]').alias('count_all'),col('coln[4]').alias('col_length'),col('coln[5]').alias('precentage_website'),\n",
    "               col('coln[6]').alias('precentage_zip'),col('coln[7]').alias('percentage_buildingCode'),col('coln[8]').alias('percentage_phoneNum'),\n",
    "               col('coln[9]').alias('percentage_lat_lon'),col('coln[10]').alias('percentage_add_st'),col('coln[11]').alias('percentage_school_name'),\n",
    "               col('coln[12]').alias('percentage_houseNo'),col('coln[13]').alias('percentage_school_lvl'),col('coln[14]').alias('percentage_person'),\n",
    "               col('coln[15]').alias('percentage_school_subject'),col('coln[16]').alias('percentage_vehicle_type'),col('coln[17]').alias('percentage_color'),\n",
    "               col('coln[18]').alias('percentage_car_make'),col('coln[19]').alias('percentage_car_model'),\n",
    "               col('coln[20]').alias('percentage_neighborhood'),col('coln[21]').alias('percentage_borough'),col('coln[22]').alias('percentage_city'),\n",
    "               col('coln[23]').alias('percentage_business_name'),col('coln[24]').alias('percentage_area_of_study'),col('coln[25]').alias('percentage_location_type'),\n",
    "               col('coln[26]').alias('percentage_parks_playgrounds'),col('coln[27]').alias('types'), col('coln[28]').alias('types_count')\n",
    "               )\n",
    "\n",
    "    types_found_count = df.where(col('types') > \" \").count()\n",
    "    print(types_found_count)\n",
    "    #df.write.csv('regex_res.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

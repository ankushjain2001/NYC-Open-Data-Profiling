{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset =========== :  1 - vw9i-7mzq.interest3.txt.gz\n",
      "0.2676056338028169\n",
      "0.04225352112676056\n",
      "+--------------+-----+\n",
      "|           val|count|\n",
      "+--------------+-----+\n",
      "|COMMUNICATIONS|    2|\n",
      "+--------------+-----+\n",
      "\n",
      "0.028169014084507043\n",
      "[['vw9i-7mzq.interest3.txt.gz', 0.29444444444444445, 0.3207935422230724, 71, 16.88888888888889, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.028169014084507043, 0.2676056338028169, 0.04225352112676056, 0, [], 24]]\n",
      "Saving Dataset =============== :  1 - vw9i-7mzq.interest3.txt.gz\n",
      "Processing Dataset =========== :  2 - tyfh-9h2y.BROOKLYN___COOPERATIVES_COMPARABLE_PROPERTIES___Building_Classification.txt.gz\n",
      "+---+-----+\n",
      "|val|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "[['tyfh-9h2y.BROOKLYN___COOPERATIVES_COMPARABLE_PROPERTIES___Building_Classification.txt.gz', 0.3785, 0.47786016085601163, 958, 10.5, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ['building_classification'], 958]]\n",
      "Saving Dataset =============== :  2 - tyfh-9h2y.BROOKLYN___COOPERATIVES_COMPARABLE_PROPERTIES___Building_Classification.txt.gz\n",
      "Processing Dataset =========== :  3 - w7w3-xahh.Location.txt.gz\n",
      "+---+-----+\n",
      "|val|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "[['w7w3-xahh.Location.txt.gz', 8.022479922576566e-05, 0.00413211825588574, 119500, 39.050562847004095, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ['lat_lon_cord'], 119500]]\n",
      "Saving Dataset =============== :  3 - w7w3-xahh.Location.txt.gz\n",
      "Processing Dataset =========== :  4 - nfkx-wd79.Address_1.txt.gz\n",
      "+---+-----+\n",
      "|val|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "[['nfkx-wd79.Address_1.txt.gz', 0.014797507788161994, 0.08923417616800781, 1983, 18.692107995846314, 0, 0, 0, 0, 0, 0.880988401412002, 0.011598587997982855, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ['address'], 1770]]\n",
      "Saving Dataset =============== :  4 - nfkx-wd79.Address_1.txt.gz\n",
      "Processing Dataset =========== :  5 - uq7m-95z8.interest1.txt.gz\n",
      "0.117096018735363\n",
      "0.0117096018735363\n",
      "0.03044496487119438\n",
      "+--------------+-----+\n",
      "|           val|count|\n",
      "+--------------+-----+\n",
      "|  ARCHITECTURE|    5|\n",
      "|COMMUNICATIONS|    7|\n",
      "+--------------+-----+\n",
      "\n",
      "0.02810304449648712\n",
      "[['uq7m-95z8.interest1.txt.gz', 0.10222727272727271, 0.21056294880927037, 427, 17.454545454545453, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0117096018735363, 0, 0, 0, 0, 0, 0, 0, 0.02810304449648712, 0.117096018735363, 0.03044496487119438, 0, [], 80]]\n",
      "Saving Dataset =============== :  5 - uq7m-95z8.interest1.txt.gz\n",
      "Processing Dataset =========== :  6 - a9md-ynri.MI.txt.gz\n",
      "0.06608911077099579\n",
      "+---+-----+\n",
      "|val|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "[['a9md-ynri.MI.txt.gz', 0.24275, 0.24266355690925628, 798422, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.06608911077099579, 0, 0, 0, 0, 0, 0, [], 52767]]\n",
      "Saving Dataset =============== :  6 - a9md-ynri.MI.txt.gz\n",
      "Processing Dataset =========== :  7 - s9d3-x4fz.EMPCITY.txt.gz\n",
      "0.2966893039049236\n",
      "0.24204159592529711\n",
      "+--------+-----+\n",
      "|     val|count|\n",
      "+--------+-----+\n",
      "|BROOKLYN|15367|\n",
      "|   CHINA|    1|\n",
      "|  GOLDEN|    1|\n",
      "|  HUDSON|   11|\n",
      "|   MEDIA|    1|\n",
      "|     NEW|   10|\n",
      "+--------+-----+\n",
      "\n",
      "0.18146340313148462\n",
      "[['s9d3-x4fz.EMPCITY.txt.gz', 0.0009195143884892062, 0.023071312640821996, 84816, 9.761690647482014, 0, 2.358045651763818e-05, 0, 0, 0, 0.0011436521411054518, 8.253159781173364e-05, 0, 0, 0, 0, 0, 0, 0, 0, 0.2966893039049236, 0, 0, 0.18146340313148462, 0, 0, 0.24204159592529711, ['neighborhood', 'parks and playgrounds', 'business'], 61190]]\n",
      "Saving Dataset =============== :  7 - s9d3-x4fz.EMPCITY.txt.gz\n",
      "Processing Dataset =========== :  8 - kz72-dump.CORE_SUBJECT___MS_CORE_and__09_12_ONLY_.txt.gz\n",
      "0.5283018867924528\n",
      "0.2641509433962264\n",
      "0.39622641509433965\n",
      "0.1320754716981132\n",
      "+---+-----+\n",
      "|val|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "[['kz72-dump.CORE_SUBJECT___MS_CORE_and__09_12_ONLY_.txt.gz', 0.2, 0.447213595499958, 53, 6.6, 0, 0, 0, 0, 0, 0.1320754716981132, 0, 0, 0, 0, 0.39622641509433965, 0, 0, 0, 0, 0, 0, 0, 0, 0.2641509433962264, 0, 0.1320754716981132, ['school subject', 'parks and playgrounds'], 49]]\n",
      "Saving Dataset =============== :  8 - kz72-dump.CORE_SUBJECT___MS_CORE_and__09_12_ONLY_.txt.gz\n",
      "Processing Dataset =========== :  9 - 4pt5-3vv4.Location.txt.gz\n",
      "+---+-----+\n",
      "|val|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "[['4pt5-3vv4.Location.txt.gz', 0.016190226650953436, 0.03395928258020093, 767156, 39.05257349450258, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ['lat_lon_cord'], 767156]]\n",
      "Saving Dataset =============== :  9 - 4pt5-3vv4.Location.txt.gz\n",
      "Processing Dataset =========== :  10 - w9ak-ipjd.Applicant_Last_Name.txt.gz\n",
      "0.0008099297385951769\n",
      "0.00040496486929758846\n",
      "0.000911170955919574\n",
      "0.00046570959969222667\n",
      "+------+-----+\n",
      "|   val|count|\n",
      "+------+-----+\n",
      "| BLACK|    2|\n",
      "| DAVID|    1|\n",
      "|GOLDEN|    2|\n",
      "|  GOOD|   15|\n",
      "| GREEN|   79|\n",
      "|JOSEPH|    1|\n",
      "|  KING|   30|\n",
      "|  PARK|    6|\n",
      "|  PAUL|    9|\n",
      "|STREET|    4|\n",
      "| URBAN|    3|\n",
      "+------+-----+\n",
      "\n",
      "0.003077733006661672\n",
      "[['w9ak-ipjd.Applicant_Last_Name.txt.gz', 0.005681902123730355, 0.03735364870870977, 49387, 6.852262234533702, 0, 0, 0, 0, 0, 0, 2.024824346487942e-05, 0, 0, 0, 0, 0, 0, 0, 0, 0.000911170955919574, 0, 0, 0.003077733006661672, 0.0008099297385951769, 0.00040496486929758846, 0.00046570959969222667, [], 281]]\n",
      "Saving Dataset =============== :  10 - w9ak-ipjd.Applicant_Last_Name.txt.gz\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- NOTES -------------------------------------------------------------------\n",
    "# 1. Update the datasets, dataList\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pyspark\n",
    "from ast import literal_eval\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.sql.functions import udf, unix_timestamp, col ,length\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, FloatType, DateType, TimestampType\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline \n",
    "from collections import Counter\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "#import spacy\n",
    "#from spacy import displacy\n",
    "#import en_core_web_sm\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# --- Function Definitions Begin ----------------------------------------------\n",
    "\n",
    "# Function to find mean and stdv for all files\n",
    "def mean_stdv(df):\n",
    "    unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "    for i in [\"count\"]:\n",
    "        assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "        scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "        pipeline = Pipeline(stages=[assembler, scaler])\n",
    "        df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "        df_stats = df.select(_mean(col('count_Scaled')).alias('mean'),_stddev(col('count_Scaled')).alias('std')).collect()\n",
    "        mean = df_stats[0]['mean']\n",
    "        std = df_stats[0]['std']\n",
    "        return df_stats \n",
    "\n",
    "# Function to sum all count of values for all files\n",
    "def count_all_values(df):\n",
    "    res = df.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "    return res\n",
    "\n",
    "# Regex function to check website type\n",
    "def re_find_website(df,count_all,found_type):\n",
    "    web_re_rexpr = \"WWW\\.|\\.COM|HTTP\\:\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(web_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"website\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function to check zip type\n",
    "def re_find_zipCode(df,count_all,found_type):\n",
    "    zip_re_rexpr = \"^\\d{5}?$|^\\d{5}?-\\d\\d\\d$|^\\d{8}?$\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(zip_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"zip_code\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function to check buildingCode type\n",
    "def re_find_buildingCode(df,count_all,found_type):\n",
    "    bc_re_rexpr = \"([A-Z])\\d\\-\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(bc_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"building_classification\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0 \n",
    "\n",
    "# Regex function to check phone number type\n",
    "def re_find_phoneNum(df,count_all,found_type):\n",
    "    phone_re_rexpr = \"^\\d{10}?$|^\\(\\d\\d\\d\\)\\d\\d\\d\\d\\d\\d\\d$|^\\d\\d\\d\\-\\d\\d\\d\\-\\d\\d\\d\\d$\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(phone_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"phone_number\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function to check lat_lon type\n",
    "def re_find_lat_lon(df,count_all,found_type):\n",
    "    ll_re_rexpr = \"\\([-+]?[0-9]+\\.[0-9]+\\,\\s*[-+]?[0-9]+\\.[0-9]+\\)\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(ll_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"lat_lon_cord\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function to check street_addrees type\n",
    "def re_find_street_address(df,count_all,col_length,found_type):\n",
    "    st_re_rexpr = \"\\sROAD|\\sSTREET|\\sPLACE|\\sDRIVE|\\sBLVD|\\sST|\\sRD|\\sDR|\\sAVENUE|\\sAVE\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(st_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.8): \n",
    "            if (col_length >= 15):\n",
    "                found_type = found_type + [\"address\"]\n",
    "            elif (col_length < 15):\n",
    "                found_type = found_type + [\"street\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function to check school name type\n",
    "def re_find_school(df,count_all,found_type):\n",
    "    school_re_rexpr = \"\\sSCHOOL|\\sACADEMY|HS\\s|ACAD|I.S.\\s|IS\\s|M.S.\\s|P.S\\s|PS\\s|ACADEMY\\s\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(school_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.5): \n",
    "            found_type = found_type + [\"school_name\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function for checking house number \n",
    "def re_find_houseNo(df,count_all,found_type):\n",
    "    houseNo_re_rexpr = \"^\\d{2}?$|^\\d{3}?$|^\\d{4}?$\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(houseNo_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"house number\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function for checking school subject\n",
    "def re_find_school_subject(df,count_all,found_type):\n",
    "    school_subj_re_rexpr = \"^ENGLISH$|^ENGLISH\\s[0-9]?$|^MATH\\s[A-Z$]|^MATH$|^SCIENCE$|^SOCIAL\\sSTUDIES$|^ALGEBRA\\s[A-Z]$|                            ^CHEMISTRY$|^ASSUMED\\sTEAM\\sTEACHING$|^EARTH\\sSCIENCE$|^GEOMETRY$|^ECONOMICS$|^GLOBAL HISTORY$|                            ^GLOBAL\\sHISTORY[A-Z]$|^LIVING ENVIRONMENT$|^PHYSICS$|^US\\sGOVERNMENT$|^US\\sGOVERNMENT$|^US\\sGOVERNMENT\\s&|                            ^US\\SHISTORY$|^GLOBAL HISTORY\\s[0-9]?$\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(school_subj_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.5): \n",
    "            found_type = found_type + [\"school subject\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# Regex function for checking school level \n",
    "def re_find_schoolLevel(df,count_all,found_type):\n",
    "    schlvl_re_rexpr = \"^[K]\\-\\d?$|^HIGH SCHOOL$|^ELEMENTARY$|^ELEMENTARY SCHOOL$|^MIDDLE SCHOOL$|^TRANSFER\\sSCHOOL$|^MIDDLE$|^HIGH\\sSCHOOL\\sTRANSFERL$|^YABC$|^[K]\\-[0-9]{2}$\"\n",
    "    df_filtered = df.filter(df[\"val\"].rlike(schlvl_re_rexpr))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        if (res >= 0.85): \n",
    "            found_type = found_type + [\"school level\"]\n",
    "        return res, found_type, count_filtered\n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "# --- Functions FOR NLP Starts HERE -------------------------------------------\n",
    "def nlp_find_person(df,count_all,found_type):\n",
    "    #Your Code HERE: \n",
    "    #Use count_all for percentage calculation\n",
    "    #Please return two values: (1)percentage of such type in this col AND (2)the type found for this column\n",
    "    #if found:\n",
    "#         found_type = found_type + [\"person\"]\n",
    "    #if not found:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_business_name(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_vehicle_type(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_color(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_car_make(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_car_model(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_neighborhood(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_borough(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "def nlp_find_city(df,count_all,found_type):\n",
    "    #Your Code HERE:\n",
    "    return 0, found_type, 0\n",
    "\n",
    "# --- Function FOR NLP End ------------------------------------------------\n",
    "\n",
    "# --- Functions FOR LIST COMPARISON Starts HERE -------------------------------\n",
    "def list_find_school_subject(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(ss_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.4): \n",
    "            found_type = found_type + [\"school subject\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "def list_find_business_name(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(biz_keywords))\n",
    "    df_filtered.show()\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.1): \n",
    "            found_type = found_type + [\"business\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "\n",
    "def list_find_neighborhood(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(nh_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.1): \n",
    "            found_type = found_type + [\"neighborhood\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "def list_find_area_of_study(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(aos_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.3): \n",
    "            found_type = found_type + [\"area of study\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "    \n",
    "\n",
    "def list_find_agency(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(ca_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.1): \n",
    "            found_type = found_type + [\"city agency\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "def list_find_location_type(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(lt_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.1): \n",
    "            found_type = found_type + [\"location type\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "def list_find_parks_playgrounds(df,count_all,found_type):\n",
    "    df_filtered = df.filter(df[\"val\"].isin(pp_keywords))\n",
    "    if (df_filtered.count() is not 0):\n",
    "        count_filtered = df_filtered.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "        res = float(count_filtered/count_all)\n",
    "        print(res)\n",
    "        if (res >= 0.1): \n",
    "            found_type = found_type + [\"parks and playgrounds\"]\n",
    "        return res, found_type, count_filtered \n",
    "    else:\n",
    "        return 0, found_type, 0\n",
    "\n",
    "def import_keyword_list(inputDir):\n",
    "    klist = sc.textFile(inputDir)\n",
    "    klist = klist.flatMap(lambda x: x.split(\",\")).collect()\n",
    "    klist = [x.strip('\"') for x in klist]\n",
    "    klist = [re.sub(\"\\[|\\]|\\'|\\'|\" \"\", \"\", item)for item in klist]\n",
    "    klist = [re.sub(\" \" \"\", \"\", item)for item in klist]\n",
    "    return(klist)\n",
    "\n",
    "def read_regex_file(inputFile):\n",
    "    with open(inputFile) as f:\n",
    "        return(f.read())\n",
    "    \n",
    "def get_regex_from_list(lst):\n",
    "    regex = \"\"\n",
    "    for word in lst:\n",
    "        regex += \"\\\\s\"\n",
    "        regex += word\n",
    "        regex += \"|\"\n",
    "    return(regex)\n",
    "\n",
    "\n",
    "\n",
    "# --- Function Definitions End ------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# --- MAIN --------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setting spark context and \n",
    "    sc = SparkContext()\n",
    "    spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"project_task2\") \\\n",
    "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "            .getOrCreate()\n",
    "    sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
    "\n",
    "\n",
    "    # Current user path\n",
    "    env_var = os.environ\n",
    "    this_user = env_var['USER']\n",
    "\n",
    "    # Input & output directories\n",
    "    inputDirectory = \"/user/hm74/NYCColumns/\"#sys.argv[1]\n",
    "    outputDirectory = \"/user/\" + this_user + \"/project/task2/\"#sys.argv[2]\n",
    "    #inputDirectory = \"/home/ted/school/big_data/project/big_data_course_project/task2/raw_data/\"\n",
    "    #inputFileClusters = \"/home/ted/school/big_data/project/big_data_course_project/task2/resources/filename_clusters.json\"\n",
    "    \n",
    "    input_pp_keywords = \"park_playground_keywords\"\n",
    "    input_aos_keywords = \"area_of_study_keywords\"\n",
    "    input_ca_keywords = \"city_agency_keywords\"\n",
    "    input_ss_keywords = \"school_subject_keywords\"\n",
    "    input_sn_keywords = \"school_name_keywords\"\n",
    "    input_lt_keywords = \"location_type_keywords\"\n",
    "    input_nh_keywords = \"neighborhood_keywords\"\n",
    "    input_biz_keywords = \"business_keywords\"\n",
    "    input_biz_keywords = \"business_keywords\"\n",
    "    \n",
    "    pp_keywords = import_keyword_list(input_pp_keywords) # parks & playgrounds\n",
    "    aos_keywords = import_keyword_list(input_aos_keywords) # area of study\n",
    "    ca_keywords = import_keyword_list(input_ca_keywords) # city agency\n",
    "    ss_keywords = import_keyword_list(input_ss_keywords) # school subject\n",
    "    sn_keywords = import_keyword_list(input_sn_keywords) # school name\n",
    "    lt_keywords = import_keyword_list(input_lt_keywords) # location type\n",
    "    nh_keywords = import_keyword_list(input_nh_keywords) # neighborhood\n",
    "    biz_keywords = import_keyword_list(input_biz_keywords) # business name \n",
    "    \n",
    "    #pp_regex = get_regex_from_list(pp_keywords)\n",
    "    #aos_regex = get_regex_from_list(aos_keywords)\n",
    "    #ca_regex = get_regex_from_list(ca_keywords)\n",
    "    #ss_regex = get_regex_from_list(ss_keywords)\n",
    "    #sn_regex = get_regex_from_list(sn_keywords)\n",
    "    #lt_regex = get_regex_from_list(lt_keywords)\n",
    "    #nh_regex = get_regex_from_list(nh_keywords)\n",
    "    #biz_regex = get_regex_from_list(biz_keywords)\n",
    "    \n",
    "    # Output JSON Semantic Schema\n",
    "    jsonSchema = {\n",
    "        \"column_name\": \"\",\n",
    "        \"semantic_type\": [],\n",
    "        \"count\": 0\n",
    "    }\n",
    "\n",
    "    # Inner semantic schema \n",
    "    semanticSchema = {\n",
    "        \"semantic_type\": \"\",\n",
    "        \"label\": \"\",\n",
    "        \"count\": 0 \n",
    "    }\n",
    "\n",
    "    # Importing cluster3 format it and put it into a list\n",
    "    #raw_data = sc.textFile(\"/user/aj2885/Project_Resource/cluster3_labels.tsv\")\n",
    "    raw_data = sc.textFile(\"true_labels.tsv\")\n",
    "    raw_list = raw_data.map(lambda x: x.split(\"\\t\")).collect()\n",
    "\n",
    "    # Iteration over dataframes begins bu using dataframe file names\n",
    "    processCount = 1\n",
    "\n",
    "    # Create schema for raw data before reading into df \n",
    "    customSchema = StructType([\n",
    "                StructField(\"val\", StringType(), True),\n",
    "                StructField(\"count\", IntegerType(), True)])\n",
    "\n",
    "\n",
    "#Testing first 10 files\n",
    "for filerow in raw_list[0:10]:\n",
    "    filename = filerow[0]\n",
    "    #filename = 'qcdj-rwhu.BUSINESS_NAME2.txt.gz'\n",
    "    if filename == 'bty7-2jhb.Owner_s_House_Zip_Code.txt.gz':\n",
    "        continue\n",
    "    labels = literal_eval(filerow[1])\n",
    "    print(\"Processing Dataset =========== : \", str(processCount) + ' - ' +filename)\n",
    "    # Read file to dataset and apply all regex functions\n",
    "    found_type = []\n",
    "    fileinfo = []\n",
    "    regex_res = []\n",
    "    df = sqlContext.read.format(\"csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").option(\"delimiter\", \"\\t\").schema(customSchema).load(inputDirectory + filename)\n",
    "    df_stats = mean_stdv(df)\n",
    "    mean = df_stats[0]['mean']\n",
    "    std = df_stats[0]['std']\n",
    "    count_all = count_all_values(df)\n",
    "\n",
    "    #added col_length which is the average length of the col\n",
    "    df_length = df.select(_mean(length(col(\"val\"))).alias('avg_length'))\n",
    "    col_length= df_length.collect()[0][0]\n",
    "\n",
    "    percentage_website, found_type, type_count_web = re_find_website(df,count_all,found_type)\n",
    "    percentage_zip, found_type, type_count_zip = re_find_zipCode(df,count_all,found_type)\n",
    "    percentage_buildingCode, found_type,type_count_building = re_find_buildingCode(df,count_all,found_type)\n",
    "    percentage_phoneNum, found_type, type_count_phone = re_find_phoneNum(df,count_all,found_type)\n",
    "    percentage_lat_lon, found_type, type_count_lat_lon = re_find_lat_lon(df,count_all,found_type)\n",
    "    percentage_add_st, found_type, type_count_add_st = re_find_street_address(df,count_all,col_length,found_type)\n",
    "    percentage_school_name, found_type, type_count_school_name= re_find_school(df,count_all,found_type)\n",
    "    percentage_house_no, found_type ,type_count_house_no= re_find_houseNo(df,count_all,found_type)\n",
    "    percentage_school_lvl, found_type, type_count_school_lvl= re_find_schoolLevel(df,count_all,found_type)\n",
    "    percentage_school_subject, found_type, type_count_school_subject= re_find_school_subject(df,count_all,found_type)\n",
    "    \n",
    "    # moved this block up here -ted\n",
    "    percentage_area_of_study, found_type, type_count_area_of_study = list_find_area_of_study(df,count_all,found_type)\n",
    "    percentage_school_subject, found_type, type_count_school_subject= list_find_school_subject(df,count_all,found_type)\n",
    "    percentage_agency, found_type, type_count_agency= list_find_agency(df,count_all,found_type)\n",
    "    percentage_location, found_type, type_count_location= list_find_location_type(df,count_all,found_type)\n",
    "    percentage_neighborhood, found_type, type_count_neighborhood= list_find_neighborhood(df,count_all,found_type)\n",
    "    percentage_parks_playgrounds, found_type, type_count_parks_playgrounds = list_find_parks_playgrounds(df,count_all,found_type)\n",
    "    percentage_business_name, found_type, type_count_business= list_find_business_name(df,count_all,found_type)\n",
    "\n",
    "    \n",
    "    type_count = type_count_web + type_count_zip + type_count_building + \\\n",
    "            type_count_phone + type_count_lat_lon + type_count_add_st + \\\n",
    "            type_count_school_name + type_count_house_no + \\\n",
    "            type_count_school_lvl + type_count_school_subject + \\\n",
    "            type_count_area_of_study + type_count_neighborhood + \\\n",
    "            type_count_agency + type_count_location + \\\n",
    "            type_count_parks_playgrounds + type_count_business\n",
    "    \n",
    "    #give a default value for all other precentages \n",
    "    percentage_person = 0\n",
    "    #percentage_business_name = 0\n",
    "    percentage_vehicle_type = 0\n",
    "    percentage_color = 0\n",
    "    percentage_car_make = 0\n",
    "    percentage_car_model = 0\n",
    "    #percentage_neighborhood = 0\n",
    "    percentage_borough= 0 \n",
    "    percentage_city = 0\n",
    "    #percentage_area_of_study = 0\n",
    "    #percentage_location = 0\n",
    "    #percentage_agency = 0\n",
    "    #percentage_parks_playgrounds = 0\n",
    "\n",
    "    #STEP TWO: NLP LABEL AND LIST CHECK\n",
    "    # if not found_type:\n",
    "    #     #ANKUSH PART: NLP CHECK TYPES\n",
    "    #     percentage_person, found_type, type_count_person = nlp_find_person(df,count_all,found_type)\n",
    "    #     percentage_business_name, found_type, type_count_business = nlp_find_business_name(df,count_all,found_type)\n",
    "    #     percentage_vehicle_type, found_type, type_count_vehicle_type = nlp_find_vehicle_type(df,count_all,found_type)\n",
    "    #     percentage_color, found_type, type_count_color = nlp_find_color(df,count_all,found_type)\n",
    "    #     percentage_car_make, found_type, type_count_car_make = nlp_find_car_make(df,count_all,found_type)\n",
    "    #     percentage_car_model, found_type, type_count_car_model = nlp_find_car_model(df,count_all,found_type)\n",
    "    #     percentage_neighborhood, found_type, type_count_neighborhood = nlp_find_neighborhood(df,count_all,found_type)\n",
    "    #     percentage_borough, found_type, type_count_borough = nlp_find_borough(df,count_all,found_type)\n",
    "    #     percentage_city, found_type, type_count_city = nlp_find_city(df,count_all,found_type)\n",
    "    \n",
    "    #     #TED PART: LIST or SIMILARITY CHECK TYPEs\n",
    "    #   percentage_school_subject, found_type, type_count_school_subject= list_find_school_subject(df,count_all,found_type)\n",
    "    #     percentage_business_name, found_type, type_count_business= list_find_business_name(df,count_all,found_type)\n",
    "    #   percentage_neighborhood, found_type, type_count_neighborhood= list_find_neighborhood(df,count_all,found_type)\n",
    "    #   percentage_area_of_study, found_type, type_count_area_of_study = list_find_area_of_study(df,count_all,found_type)\n",
    "    #   percentage_agency, found_type, type_count_agency= list_find_agency(df,count_all,found_type)\n",
    "    #   percentage_location, found_type, type_count_location= list_find_location_type(df,count_all,found_type)\n",
    "    #   percentage_parks_playgrounds, type_count_location_parks_playgrounds = list_find_parks_playgrounds(df,count_all,found_type\n",
    "    # !!! NOTE: Please remeber to add type_count_XXX back to type_count in LINE 347\n",
    "    fileinfo.extend([filename,mean,std,count_all,col_length, percentage_website, percentage_zip,percentage_buildingCode,percentage_phoneNum,percentage_lat_lon,percentage_add_st,percentage_school_name,percentage_house_no,percentage_school_lvl,percentage_person,percentage_school_subject,percentage_vehicle_type, percentage_color,percentage_car_make,percentage_car_model,percentage_neighborhood,percentage_borough,percentage_city,percentage_business_name,percentage_area_of_study,percentage_location,percentage_parks_playgrounds,found_type, type_count])\n",
    "    regex_res.append(fileinfo)\n",
    "    print(regex_res)\n",
    "    # USE ME to export the JSON for current dataset\n",
    "    print(\"Saving Dataset =============== : \", str(processCount) + ' - ' +filename)\n",
    "    processCount += 1\n",
    "    #outJSON = deepcopy(jsonSchema)\n",
    "    #outJSON[\"column_name\"] = filename\n",
    "    #outJSON[\"semantic_type\"] = found_type\n",
    "    #outJSON[\"count\"] = type_count\n",
    "    #outJSON = sc.parallelize([json.dumps(outJSON)])\n",
    "    #outJSON.saveAsTextFile(outputDirectory + filename + '/task2.json')\n",
    "\n",
    "\n",
    "\n",
    "# Output regex function result \n",
    "rdd = sc.parallelize(regex_res)\n",
    "row_rdd = rdd.map(lambda x: Row(x))\n",
    "df = row_rdd.toDF()\n",
    "df = df.select(col('_1').alias('coln'))\n",
    "length = len(df.select('coln').take(1)[0][0])\n",
    "df = df.select([df.coln[i] for i in range(length)])\n",
    "df = df.select(col('coln[0]').alias('filename'),col('coln[1]').alias('mean'),col('coln[2]').alias('stdv'),\n",
    "           col('coln[3]').alias('count_all'),col('coln[4]').alias('col_length'),col('coln[5]').alias('precentage_website'),\n",
    "           col('coln[6]').alias('precentage_zip'),col('coln[7]').alias('percentage_buildingCode'),col('coln[8]').alias('percentage_phoneNum'),\n",
    "           col('coln[9]').alias('percentage_lat_lon'),col('coln[10]').alias('percentage_add_st'),col('coln[11]').alias('percentage_school_name'),\n",
    "           col('coln[12]').alias('percentage_houseNo'),col('coln[13]').alias('percentage_school_lvl'),col('coln[14]').alias('percentage_person'),\n",
    "           col('coln[15]').alias('percentage_school_subject'),col('coln[16]').alias('percentage_vehicle_type'),col('coln[17]').alias('percentage_color'),\n",
    "           col('coln[18]').alias('percentage_car_make'),col('coln[19]').alias('percentage_car_model'),\n",
    "           col('coln[20]').alias('percentage_neighborhood'),col('coln[21]').alias('percentage_borough'),col('coln[22]').alias('percentage_city'),\n",
    "           col('coln[23]').alias('percentage_business_name'),col('coln[24]').alias('percentage_area_of_study'),col('coln[25]').alias('percentage_location_type'),\n",
    "           col('coln[26]').alias('percentage_parks_playgrounds'),col('coln[27]').alias('types'), col('coln[28]').alias('types_count')\n",
    "           )\n",
    "\n",
    "types_found_count = df.where(col('types') > \" \").count()\n",
    "print(types_found_count)\n",
    "#df.write.csv('regex_res.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'business_keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-abe354e31139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbusiness_keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'business_keywords' is not defined"
     ]
    }
   ],
   "source": [
    "business_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
